# Smart Web Apps - 08: Real-Time AI Applications: Predictive Analytics

## üîÆ Introduction: Advancing Toward Predictive Intelligence in Web Applications

In our prior exploration within the _Smart Web Apps_ series, we examined the transformative impact of conversational AI‚Äîsystems designed to enable responsive, real-time interactions grounded in user intent. Building upon that foundation, this chapter represents a paradigmatic evolution: a shift from reactive communication to **proactive computational foresight**. This is the domain of **predictive intelligence**, where digital systems are no longer passive instruments but active agents anticipating user needs, behavioral trajectories, and emergent risks.

Predictive intelligence is operationalized through **predictive analytics**, an interdisciplinary approach that leverages historical and real-time data to model probable future outcomes. Its applications permeate every domain‚Äîfrom e-commerce demand forecasting and healthcare risk stratification to financial anomaly detection and user retention modeling in SaaS platforms. Increasingly, predictive analytics is not merely an analytical add-on but a strategic core of scalable, data-driven architectures.

For web developers, embracing this paradigm requires a fundamental shift in design methodology. Web systems are evolving from statically rendered interfaces to **intelligent orchestration layers** capable of autonomous insight generation and real-time decision support. Imagine a logistics dashboard that proactively adjusts delivery schedules in response to weather forecasts, or a financial platform that surfaces anomalies prior to market disruption. In these instances, prediction augments perception, transforming dashboards into **anticipatory systems** with operational autonomy.

To enable such intelligent functionality, developers must master a diverse array of tools, models, and deployment strategies. This article will explore in-depth:

- The theoretical underpinnings and practical applications of predictive analytics, with emphasis on time series modeling and sequential data learning.
- Comparative insights into classical statistical techniques (e.g., ARIMA, Holt-Winters, Seasonal Decomposition) versus deep learning paradigms (e.g., Long Short-Term Memory [LSTM] networks, Temporal Convolutional Networks [TCNs]).
- Implementation architectures for deploying predictive models in real-time web environments, utilizing frameworks such as TensorFlow, Apache Kafka, Redis Streams, and FastAPI.
- Interactive data visualization methodologies using advanced libraries including Plotly, D3.js, and Apache ECharts for rendering actionable insights via dynamic dashboards.

This exposition is intended not merely to present a toolchain, but to provoke a deeper architectural consciousness‚Äîone that views predictive functionality as intrinsic to the design of web-native systems. Our aim is to catalyze a new era in web development, where systems are **not only reactive but perceptive**, not just analytical but anticipatory.

As we navigate this landscape together, we encourage you to envision the next generation of digital experiences: platforms that do not simply respond to user prompts, but intuitively forecast, adapt, and evolve. This is the essence of predictive intelligence, and it marks a pivotal frontier in the ongoing transformation of the web into a medium of intelligent, autonomous computation. üöÄ

## üìä Understanding Predictive Analytics

Predictive analytics stands as a foundational pillar within advanced data science and intelligent systems design, concerned with the probabilistic estimation of future outcomes by synthesizing historical, real-time, and contextual data streams. It combines methodologies from statistical inference, machine learning, and time series analysis to generate actionable insights that support anticipatory decision-making and autonomous system adaptation. Distinct from retrospective forms of analysis‚Äînamely, **descriptive analytics**, which summarizes past events, and **diagnostic analytics**, which elucidates causal relationships‚Äîpredictive analytics is oriented toward forecasting, preemptive mitigation, and strategic foresight.

### üß† Differentiating Analytical Paradigms: From Description to Prediction

Descriptive analytics addresses the query _What happened?_, aggregating data to reveal patterns and summaries of historical behavior. Diagnostic analytics goes further, probing _Why did it happen?_ through techniques such as multivariate analysis, regression diagnostics, and root cause evaluation.

Predictive analytics, in contrast, seeks to answer _What will happen next, and how confident are we in that forecast?_ It utilizes a range of techniques‚Äîfrom classical time series models to contemporary deep learning approaches‚Äîto model and extrapolate trends, anomalies, and latent patterns. This paradigm repositions data as a dynamic forecasting engine, transforming past performance into predictive capability.

### ‚öôÔ∏è Temporal Architectures: Batch vs. Real-Time Inference Pipelines

Legacy predictive workflows have traditionally relied on batch processing, in which data is ingested, transformed, and analyzed on scheduled intervals. While this is sufficient for long-range forecasting in relatively static environments, it lacks the temporal sensitivity required by today‚Äôs high-frequency, event-driven applications.

Modern systems are evolving toward **real-time predictive architectures**. These systems continuously ingest streaming data, often from heterogeneous sources, and process it asynchronously using platforms such as Apache Kafka, Apache Flink, and Spark Streaming. They are coupled with low-latency model inference engines like TensorFlow Serving, TorchServe, and ONNX Runtime to deliver sub-second predictions. These capabilities support a wide array of dynamic use cases, including live fraud detection, in-session personalization, and real-time anomaly flagging.

The operational significance of real-time systems is substantial‚Äîthey enable adaptive feedback loops, facilitate online learning, and maintain system responsiveness under volatile conditions. As such, they represent a critical component in the infrastructure of intelligent, autonomous digital ecosystems.

### üåê Domain-Specific Applications of Predictive Intelligence

Predictive analytics is reshaping operational paradigms across industries by embedding anticipatory capabilities directly into core systems and workflows. Notable applications include:

- **Revenue Forecasting:** Leveraging econometric models and time series decomposition to estimate sales trajectories and optimize pricing strategies.
- **Inventory and Supply Chain Optimization:** Utilizing probabilistic demand forecasts to align inventory levels with consumption patterns and supplier constraints.
- **Healthcare Diagnostics and Prognostics:** Integrating biometric data, clinical records, and wearable signals to forecast disease onset and personalize interventions.
- **Churn and Behavioral Risk Scoring:** Applying temporal behavior modeling to identify disengaged users or preempt attrition in subscription-based services.
- **Cybersecurity and Threat Mitigation:** Detecting anomalies in real-time user behavior and access patterns to prevent system breaches or fraudulent activities.
- **Predictive Maintenance and IoT Monitoring:** Analyzing telemetry data to forecast equipment degradation and schedule preemptive maintenance.
- **Personalization Engines:** Adapting content delivery, UI configurations, and interaction flows based on predicted user preferences and intent.

These implementations underscore the epistemic and operational utility of predictive analytics. Far from a peripheral capability, it has become central to the **cognitive infrastructure** of contemporary digital platforms, enabling systems to operate not just reactively, but proactively.

In the subsequent section, we will examine the algorithmic underpinnings of time series forecasting‚Äîan essential method for constructing robust, temporally aware predictive models. These methodologies will provide the foundation for implementing scalable, adaptive platforms that respond intelligently to both user behavior and environmental dynamics.

## ‚è≥ Time Series Forecasting: Foundations and Theoretical Considerations

Time series forecasting constitutes a foundational discipline within predictive modeling, offering essential methodologies for projecting temporally dependent phenomena. Unlike conventional supervised learning tasks that presume independent and identically distributed observations, time series data inherently exhibit sequential dependency, temporal autocorrelation, and often complex patterns of seasonality and non-stationarity. These properties demand a specialized analytic lens capable of modeling dynamic temporal behaviors. As real-time inference becomes increasingly central in domains such as financial forecasting, clinical prognosis, and supply chain optimization, a rigorous understanding of time series dynamics is indispensable for both academic researchers and industry practitioners.

This section presents an exposition of core time series principles. It delineates the structural components of temporal data, articulates the methodological limitations of conventional machine learning algorithms, and provides a comprehensive framework for exploratory data analysis (EDA) in time series contexts.

### üîç Structural Decomposition: Trend, Seasonality, Cyclicality, and Stochasticity

Time series data are composed of multiple underlying components, each representing a distinct mode of variation. Decomposing a series into these components facilitates interpretability, improves forecasting accuracy, and enables robust diagnostic evaluation. The primary constituents include:

- **Trend:** A long-term directional component‚Äîeither deterministic or stochastic‚Äîcapturing the overarching movement of the series. Trends typically reflect systemic macroeconomic, technological, or environmental shifts.

- **Seasonality:** Regular and predictable patterns occurring at fixed temporal intervals. Seasonality often stems from institutional or environmental cycles (e.g., weekly sales, monthly demand spikes) and may exhibit additive or multiplicative characteristics.

- **Cyclicality:** Oscillatory patterns with variable frequency and amplitude that result from endogenous system dynamics. Unlike seasonality, cycles are irregular and context-dependent, requiring more flexible modeling strategies.

- **Noise (Irregular Component):** The stochastic residual that remains after accounting for structural variation. Accurate modeling of this component is critical for quantifying forecast uncertainty and ensuring statistical rigor.

Advanced decomposition techniques‚Äîsuch as STL (Seasonal and Trend decomposition using Loess) and X-13ARIMA-SEATS‚Äîallow for precise separation of these elements and support model specification and parameter calibration.

### ‚ö†Ô∏è Methodological Limitations of Conventional Machine Learning for Sequential Data

While conventional machine learning models such as random forests, support vector machines (SVMs), and gradient boosting frameworks have achieved widespread success in static tabular domains, they are not inherently equipped to model sequential dependencies. Key limitations include:

- **Temporal Ignorance:** These models typically disregard the order of observations, necessitating extensive manual feature engineering to introduce lag structures and temporal context.

- **Assumption of Stationarity:** Traditional models often assume stability in the data-generating process, which is frequently violated in real-world time series subject to drift, regime change, or seasonal perturbation.

- **Limited Forecast Horizon Capability:** Many models are optimized for single-step predictions and lack architectures for recursive or direct multi-step forecasting.

- **Extensive Feature Construction Requirements:** Practitioners must manually engineer features to represent temporal dependencies‚Äîe.g., lag variables, rolling statistics, temporal embeddings‚Äîintroducing additional design complexity and risk of information leakage.

In light of these constraints, time series-specific methodologies‚Äîsuch as autoregressive integrated moving average (ARIMA), exponential smoothing models, and deep learning architectures like LSTMs and TCNs‚Äîoffer more appropriate frameworks for modeling temporal dynamics.

### üß™ Temporal Exploratory Data Analysis (EDA): Strategies and Diagnostics

Exploratory data analysis serves as the analytical foundation for time series modeling, enabling practitioners to characterize temporal structure, assess statistical assumptions, and inform downstream methodological decisions. Essential EDA techniques include:

- **Visual Inspection and Rolling Statistics:** Line plots augmented with rolling means, medians, and standard deviations highlight non-stationarity, volatility shifts, and trend transitions.

- **Time Series Decomposition:** Algorithms such as STL and classical decomposition isolate trend, seasonal, and irregular components, enhancing interpretability and aiding in model selection.

- **Autocorrelation Diagnostics:** Autocorrelation function (ACF) and partial autocorrelation function (PACF) plots identify significant lags and guide the specification of autoregressive and moving average terms.

- **Stationarity Assessment:** Tests such as the Augmented Dickey-Fuller (ADF), KPSS, and Phillips-Perron evaluate the constancy of statistical properties over time‚Äîa prerequisite for many classical models.

- **Residual Analysis:** Post-modeling diagnostics‚Äîvia residual plots, histograms, and Q-Q plots‚Äîverify normality, independence, and homoscedasticity, essential for validating forecast quality.

- **Lag and Seasonal Subseries Plots:** These visualizations uncover recurring seasonal behavior and nonlinear relationships, revealing structure that may be obscured in aggregate.

When executed rigorously, EDA enables the construction of informed priors, justifies preprocessing decisions (e.g., differencing, transformation), and lays the groundwork for developing statistically sound and operationally viable forecasting models.

In the next section, we transition from exploratory methods to formalized modeling strategies. We will begin with classical approaches‚Äînamely, ARIMA, exponential smoothing, and state space models‚Äîbefore advancing to the architecture and implementation of deep learning methods specifically designed for temporal inference.

## üìâ Classical Models for Time Series Forecasting

Classical statistical models continue to play a vital role in the field of time series forecasting, despite the increasing dominance of deep learning methods. Their enduring utility is rooted in a unique combination of theoretical clarity, computational efficiency, and interpretability. These models excel in scenarios characterized by relatively stable temporal dynamics, limited data availability, and environments that demand transparency‚Äîsuch as regulated industries or academic research. This section presents a scholarly overview of two principal classes of classical time series models‚ÄîARIMA and seasonal decomposition‚Äîand delineates conditions under which these approaches may outperform more sophisticated machine learning techniques.

### üîÅ ARIMA: Autoregressive Integrated Moving Average

ARIMA (Autoregressive Integrated Moving Average) remains one of the most canonical models in univariate time series forecasting. It provides a flexible framework that models time series data using three core operations:

- **Autoregression (AR):** Captures linear dependence on previous time steps, expressing the current value as a function of past observations.
- **Integration (I):** Implements differencing to remove trends or unit roots, ensuring the series exhibits stationarity‚Äîan essential assumption for many time series models.
- **Moving Average (MA):** Models the error term as a function of previous forecasting errors, smoothing irregularities in the residuals.

The model is typically represented as ARIMA(p, d, q), where:

- **p** denotes the order of the autoregressive component,
- **d** specifies the degree of differencing,
- **q** represents the order of the moving average component.

Model building involves several methodical steps:

- Visualization and transformation (e.g., logarithmic or Box-Cox transformation);
- Stationarity assessment using unit root tests such as the Augmented Dickey-Fuller (ADF), KPSS, or Phillips-Perron;
- Autocorrelation and partial autocorrelation analysis (ACF and PACF) to inform parameter selection;
- Selection of optimal model using criteria like AIC and BIC;
- Residual diagnostics to verify assumptions of white noise and absence of serial correlation.

#### ARIMA Extensions

- **SARIMA:** Incorporates seasonal autoregressive, differencing, and moving average terms, parameterized as ARIMA(p, d, q)(P, D, Q)\_s.
- **ARIMAX:** Enhances ARIMA by introducing exogenous covariates for conditional modeling.

ARIMA and its variants are particularly effective for short-term forecasting tasks in domains such as finance, economics, and environmental science, where model interpretability and rigorous statistical inference are paramount.

### üìà Seasonal Decomposition Techniques

Seasonal decomposition methods aim to disentangle a time series into interpretable sub-components, typically trend, seasonality, and residual (irregular) elements. This decomposition enhances interpretability, supports preprocessing, and informs model specification.

#### Major Decomposition Techniques

- **Classical Decomposition:** Based on moving averages, this method assumes additive or multiplicative structure and works best with regular seasonality.
- **STL (Seasonal-Trend decomposition via Loess):** A robust and flexible method using locally weighted regression, ideal for handling non-linear trends and complex seasonality.
- **X-13ARIMA-SEATS:** A powerful method for seasonal adjustment, developed by the U.S. Census Bureau, which integrates ARIMA modeling with regression-based decomposition.

Decomposition is valuable for isolating underlying components of interest, facilitating anomaly detection, improving feature engineering, and enabling hybrid modeling strategies.

### üîç When Classical Models Excel Over Deep Learning

Although deep learning methods such as recurrent neural networks (RNNs), temporal convolutional networks (TCNs), and attention-based transformers offer superior capacity for modeling complex, high-dimensional, and nonlinear systems, classical models retain competitive advantages in specific contexts:

- **Limited Data Availability:** Classical models like ARIMA can be effectively trained on small datasets, whereas neural networks typically require extensive data for generalization.
- **Short-Term Forecasting:** For low-horizon predictions, classical models frequently yield accurate forecasts with minimal computational overhead.
- **Stable or Stationary Processes:** In settings characterized by linearly evolving dynamics, classical models often perform as well as or better than deep models with less complexity.
- **Explainability Requirements:** In domains such as healthcare, finance, and legal systems, model transparency is not only preferred but often mandated.
- **Efficiency and Prototyping:** Classical models are easy to implement, fast to train, and highly suitable for rapid experimentation or resource-constrained environments.

In sum, classical methods serve not only as foundational models and robust baselines but also as practical solutions in many real-world applications. Their transparent structure, minimal hyperparameter tuning, and ability to yield statistically rigorous forecasts make them indispensable in the modern forecasting arsenal. In the following section, we explore how deep learning models enhance forecasting capabilities by overcoming the structural assumptions and limitations inherent in classical techniques.

## üß† Deep Learning for Time Series Forecasting

Deep learning has revolutionized time series forecasting by introducing computational models capable of capturing intricate temporal dependencies, nonlinear dynamics, and high-dimensional interactions beyond the expressive capacity of classical statistical frameworks. Unlike traditional models, which often assume linearity, stationarity, and homoscedasticity, deep neural networks enable a data-driven paradigm that autonomously learns representational hierarchies from raw sequential data, thereby circumventing the limitations imposed by manual feature engineering.

The convergence of algorithmic innovations, abundant time-stamped data, and scalable compute infrastructure‚Äîspanning GPUs, TPUs, and distributed training frameworks‚Äîhas facilitated the widespread adoption of deep learning in critical forecasting domains such as quantitative finance, energy grid optimization, biomedical signal processing, and real-time monitoring systems. In this section, we examine the most salient deep learning architectures used in time series analysis: Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) units, and Temporal Convolutional Networks (TCNs). We conclude with a comparative assessment of these methods vis-√†-vis classical forecasting models.

### üîÑ Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)

RNNs form the conceptual backbone of early deep learning approaches for sequential modeling. Their defining characteristic is the use of internal hidden states that are updated recursively across time steps, allowing the network to model temporal dependencies. However, standard RNNs are hindered by the vanishing and exploding gradient problem during backpropagation through time (BPTT), limiting their utility for capturing long-range dependencies.

LSTMs were introduced to mitigate these issues. They incorporate specialized gating mechanisms‚Äînamely input, forget, and output gates‚Äîwhich regulate the flow of information across time. These gates endow LSTMs with the ability to retain relevant temporal context over extended sequences, making them highly effective for a wide range of forecasting applications.

#### Representative Applications:

- **Financial Markets:** Algorithmic trading, risk forecasting, asset allocation.
- **Smart Grids:** Demand forecasting, load balancing, outage prediction.
- **Sequential NLP:** Temporal topic evolution, user behavior modeling.
- **Industrial Systems:** Equipment health monitoring, predictive maintenance.

### üåä Temporal Convolutional Networks (TCNs)

TCNs offer a compelling alternative to recurrent architectures by employing causal and dilated convolutions for sequence modeling. Causality ensures that the output at any time \( t \) depends solely on current and previous inputs, while dilation exponentially increases the network‚Äôs receptive field, allowing it to capture long-range dependencies with fewer layers.

Because convolutions are inherently parallelizable, TCNs enable faster training and inference compared to RNNs. Their stability during optimization and capacity to model hierarchical temporal features make them particularly well-suited for applications requiring high throughput and low latency.

#### Architectural Strengths:

- **Parallel Computation:** Efficient training on large-scale time series.
- **Temporal Scalability:** Flexible handling of short- and long-term patterns.
- **Noise Robustness:** Resilience to missing, irregular, or corrupted data.
- **Stream Processing:** Suitable for real-time analytics and event-driven systems.

### ‚öñÔ∏è Comparative Assessment: Deep Learning vs Classical Time Series Models

While classical models such as ARIMA, Holt-Winters, and state space formulations offer robust performance in low-dimensional, stationary settings with small datasets, they lack the capacity to capture complex temporal hierarchies and nonlinear dependencies inherent in many real-world phenomena.

#### Deep Learning Advantages:

- **Automatic Feature Learning:** Reduces the need for domain-specific feature engineering.
- **Capacity for Nonlinearity:** Captures chaotic, multiscale, and high-order interactions.
- **Cross-Modal Integration:** Fuses heterogeneous data modalities in unified models.
- **Model Reusability:** Enables transfer learning and domain adaptation.

#### Limitations and Considerations:

- **Lack of Interpretability:** Black-box behavior limits use in regulated environments.
- **Data Requirements:** Generalization depends on access to large, diverse datasets.
- **Training Complexity:** Demands considerable computational and tuning effort.
- **Overfitting Potential:** Sensitive to dataset imbalance and low signal-to-noise ratios.

In summary, deep learning has significantly expanded the methodological toolkit for time series forecasting. It is particularly advantageous in domains characterized by complexity, data abundance, and the need for flexible, high-capacity models. However, its deployment must be accompanied by rigorous validation, thoughtful model selection, and integration with domain knowledge. In the forthcoming section, we will examine hybrid and ensemble approaches that blend the interpretability of classical techniques with the representational power of deep neural networks, fostering solutions that are both robust and explainable.

## ‚öôÔ∏è Preparing Data for Real-Time Inference

Real-time machine learning (ML) systems demand the agile acquisition, transformation, and delivery of temporally ordered data under stringent latency and reliability constraints. The preprocessing layer within such environments is foundational‚Äîit directly governs the responsiveness of model inference, the semantic fidelity of feature representation, and the resilience of downstream prediction pipelines. Unlike batch-based paradigms, which offer relaxed computational and temporal guarantees, real-time ML workflows require meticulously engineered data pipelines that are performant, temporally coherent, and capable of seamless integration into continuous deployment workflows.

This section offers a comprehensive overview of real-time data pipeline architectures, delineates key strategies for temporally aware feature construction, and surveys the technological landscape supporting real-time ingestion and orchestration at scale.

### üîÑ Real-Time Data Pipeline Architectures: Streaming vs. Microbatching

The structural design of real-time data pipelines typically adheres to one of two operational modalities: **streaming** or **microbatching**. Each presents trade-offs that must be assessed in relation to the domain-specific requirements for throughput, latency, and system complexity.

- **Streaming Pipelines:** These pipelines process events individually and continuously upon ingestion. They are indispensable in latency-critical applications such as algorithmic trading, real-time fraud detection, and industrial automation. Designing robust streaming pipelines necessitates rigorous attention to message ordering, watermarking for event-time alignment, and exactly-once state consistency. Frameworks like Apache Flink and Kafka Streams exemplify this paradigm, providing sophisticated capabilities for stateful stream computation and fault-tolerant recovery.

- **Microbatching Pipelines:** This architecture aggregates incoming data over short, fixed intervals‚Äîtypically on the order of seconds‚Äîand processes it as discrete mini-batches. While marginally less responsive than streaming, it offers improved throughput and simplified error handling. Apache Spark Structured Streaming is the archetypal platform for this approach, affording strong integration with batch workflows and compatibility with distributed storage layers.

Increasingly, hybridized systems combine these paradigms, leveraging streaming for mission-critical inference and microbatching for analytics, retraining, and monitoring.

### üßÆ Temporal Feature Engineering: Constructing Lag-Aware Representations

In real-time systems, the fidelity and relevance of features are tightly coupled to their temporal alignment and statistical expressiveness. Feature engineering must be efficient, drift-resilient, and precisely synchronized between training and inference.

#### Key Techniques:

- **Lag Features:** Represent time-lagged observations of key variables to encode autoregressive patterns. Their implementation typically relies on stateful buffers, often with entity-level granularity.
- **Rolling Statistics:** Capture evolving trends and variability by computing windowed aggregations such as moving averages, standard deviations, and quantiles.
- **Cyclical Time Encodings:** Use sine and cosine transformations to embed features like hour-of-day or day-of-week, preserving temporal periodicity.
- **Event-Based Indicators:** Derive binary or categorical flags based on operational thresholds, regime transitions, or domain-specific conditions.

Ensuring feature parity between the training pipeline and the production inference stack is essential. This often necessitates adopting real-time feature stores (e.g., Feast) or implementing low-latency, deterministic computation layers colocated with model serving infrastructure.

### ‚ö° Real-Time Ingestion and Orchestration Tooling

The success of real-time inference systems hinges on the seamless integration of ingestion frameworks, stream processors, and orchestration engines capable of supporting high-throughput, low-latency workflows.

- **Apache Kafka:** A fault-tolerant distributed log that facilitates real-time event ingestion and inter-service communication. With extensions like Kafka Streams, it supports native stream transformations and stateful processing.
- **Apache Flink:** A highly expressive event-time stream processor supporting advanced state management, low-latency processing, and complex temporal joins.
- **Apache Spark Structured Streaming:** Offers a unified batch-streaming interface well-suited for microbatch ETL, real-time aggregations, and model scoring pipelines.
- **Apache Airflow:** Although not optimized for sub-second execution, it remains a premier tool for orchestrating retraining workflows, periodic recomputation of features, and SLA-governed processes.
- **Cloud-Native Solutions (e.g., GCP Dataflow, AWS Kinesis, Azure Stream Analytics):** Managed services that offer scalable, autoscaling stream processing with built-in observability, fault tolerance, and seamless integration into cloud-native ML pipelines.

Tool selection should reflect the system‚Äôs requirements for latency sensitivity, scalability, fault tolerance, state management complexity, and ease of integration with serving infrastructure. Key evaluation metrics include end-to-end processing latency, message replay support, checkpointing granularity, and SLA compliance.
