# Conversational AI: Building Intelligent Chatbots Across Platforms - 03: Hands-On: Build a Website Chatbot with OpenAI + LangChain

## ğŸ¤– Project Overview: What Weâ€™re Building

In this tutorial, we're diving into the practical side of conversational AI by building a **full-stack website chatbot** using **LangChain** and **OpenAI**, entirely on the **free tier**. This project is designed to help you understand how to integrate large language models into real-world applications without relying on paid tools. By the end, you'll have a fully functional chatbot that can hold multi-turn conversations directly on your website.

### ğŸ¯ Chatbot Use Case: Conversational Website Assistant

The bot we'll build acts like a customer-facing assistant capable of:

- Holding a conversation (with memory)
- Answering questions based on previous inputs
- Responding naturally using an LLM like GPT-3.5 or open-source alternatives
- Running entirely within a free deployment setup (Railway + Vercel)

Think of it as a **starter template** for:

- Support bots for SaaS
- Product FAQs bots
- Personal portfolio site assistants
- E-learning bots for documentation

### ğŸ§± Project Stack Breakdown

Hereâ€™s the tech stack weâ€™ll use:

#### Backend

- **Node.js + Express** â€” lightweight API server
- **LangChain (JS)** â€” orchestrates the LLM + memory + chains
- **OpenAI / Hugging Face** â€” for the LLM interface (we'll support both)
- **BufferMemory** â€” to track past user-bot interactions in memory

#### Frontend

- **React + TailwindCSS** â€” clean, minimal UI for the chatbot
- Fetch-based API calls to our backend

#### Deployment

- **Vercel** (Frontend)
- **Railway** (Backend)
- GitHub for version control and CI/CD

### ğŸ’¸ Free-Tier Strategy: Cost-Conscious from the Ground Up

This project is built for developers who want to:

- Avoid vendor lock-in
- Experiment without needing a credit card
- Prototype with generous free usage limits

Hereâ€™s how we handle common cost-related blockers:

| Concern    | Free-Tier Solution                                                   |
| ---------- | -------------------------------------------------------------------- |
| LLM Usage  | OpenAI free-tier key or Hugging Face API (text-generation-inference) |
| Vector DB  | `Chroma` - local, embedded vector store                              |
| Deployment | Vercel (free hobby plan) + Railway (free backend instance)           |

Youâ€™ll be able to run this bot **end-to-end for \$0/month**.

### ğŸ§© Architecture Preview

The high-level flow looks like this:

```plaintext
User Input -> React Frontend -> Express API -> LangChain LLM Chain -> LLM Response -> Back to User
```

- Memory is preserved in the backend per session (in-process for now)
- Optional file upload uses LangChain's RAG tools with Chroma (all local)

### ğŸ“ Folder Structure (Planned)

Weâ€™ll structure our project clearly for modularity:

```plaintext
/chatbot-openai-langchain-starter
|-- client/            # React frontend
|-- server/            # Express + LangChain backend
|   |-- routes/
|   |-- llm/
|   |-- memory/
|-- .env               # For API keys
|-- README.md          # Setup instructions
```

### ğŸš€ Next Steps

Now that we know what weâ€™re building and how everything fits together, itâ€™s time to get our hands dirty. In the next section, weâ€™ll:

- Scaffold the backend
- Set up LangChain and connect to an LLM
- Write our first API route

Letâ€™s go build a chatbot, shall we? ğŸ› ï¸

## âš™ï¸ Setting Up the Backend: LangChain + Express API

Letâ€™s kick things off by building the backend for our chatbot. Weâ€™ll use **Node.js + Express** as our server and plug in **LangChain** to handle language model interactions. This section walks you through the exact steps â€” code-first, no assumptions. Ready to build? Letâ€™s go ğŸ› ï¸

### ğŸ“ Project Structure

Before diving into code, hereâ€™s how weâ€™ll organize our project. Keeping a clean folder structure helps with scalability and debugging.

```plaintext
chatbot-openai-langchain-starter/
â”œâ”€â”€ server/                 # Backend API
â”‚   â”œâ”€â”€ index.js            # Main entry point
â”‚   â”œâ”€â”€ routes/             # API routes
â”‚   â”‚   â””â”€â”€ chat.js         # Chat endpoint logic
â”‚   â”œâ”€â”€ llm/                # LangChain-related logic
â”‚   â”‚   â””â”€â”€ langchain.js    # Chain + memory config
â”‚   â””â”€â”€ .env                # Environment variables
â”œâ”€â”€ client/                 # Frontend (will be added later)
â””â”€â”€ README.md               # Documentation
```

### ğŸ§© Step 1: Initialize the Project

Start by setting up your project folder and backend workspace:

```bash
mkdir chatbot-openai-langchain-starter && cd chatbot-openai-langchain-starter
mkdir server && cd server
npm init -y
```

This sets up a new Node.js project inside the `server` directory.

### ğŸ“¦ Step 2: Install Dependencies

We need these packages:

- `express`: Web server
- `dotenv`: Manage environment variables
- `cors`: Allow frontend to call backend
- `langchain`: Power LLM chains and memory
- `openai`: SDK for calling OpenAI models

Install everything in one go:

```bash
npm install express dotenv cors langchain openai
```

Enable ES module support by editing `package.json`:

```json
{
  "type": "module"
}
```

This allows us to use `import`/`export` instead of `require()`.

### ğŸ› ï¸ Step 3: Basic Express Server

Now weâ€™ll set up our basic API server that will act as a middleman between the frontend and the LLM.

Create `server/index.js`:

```js
import express from "express";
import dotenv from "dotenv";
import cors from "cors";
import chatRoute from "./routes/chat.js";

dotenv.config();

const app = express();
const PORT = process.env.PORT || 3001;

app.use(cors());
app.use(express.json());

app.use("/chat", chatRoute);

app.get("/", (req, res) => {
  res.send("Chatbot backend is running");
});

app.listen(PORT, () => {
  console.log(`Server listening on http://localhost:${PORT}`);
});
```

This initializes the server and mounts our chatbot API at `/chat`.

### ğŸ§  Step 4: Create the Chat Route

Weâ€™ll now define the POST route where incoming user messages are received and passed to the LLM.

Create `server/routes/chat.js`:

```js
import express from "express";
import { runChat } from "../llm/langchain.js";

const router = express.Router();

router.post("/", async (req, res) => {
  const { message } = req.body;
  if (!message) {
    return res.status(400).json({ error: "Message is required" });
  }
  try {
    const response = await runChat(message);
    res.json({ response });
  } catch (err) {
    console.error("Chat error:", err);
    res.status(500).json({ error: "Something went wrong" });
  }
});

export default router;
```

This is our `/chat` endpoint â€” it accepts a user message, passes it to LangChain, and returns the response.

### ğŸ¤– Step 5: LangChain Integration

This is where we bring in the power of LLMs.

Create `server/llm/langchain.js`:

```js
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { BufferMemory } from "langchain/memory";

const model = new ChatOpenAI({
  temperature: 0.7,
  openAIApiKey: process.env.OPENAI_API_KEY,
});

const memory = new BufferMemory();
const chain = new ConversationChain({ llm: model, memory });

export async function runChat(input) {
  const result = await chain.call({ input });
  return result.response;
}
```

Hereâ€™s what this setup does:

- Connects to OpenAI using `ChatOpenAI`
- Adds short-term memory via `BufferMemory`
- Wraps both in a `ConversationChain`, which is a dialogue loop
- Exposes `runChat()` that accepts user input and returns a response

### ğŸ” Step 6: Setup Environment Variables

Inside your `server/` folder, create a `.env` file:

```env
OPENAI_API_KEY=your_openai_key_here
```

This keeps sensitive credentials out of your source code.

### âœ… Step 7: Run and Test Your Backend

Start your server:

```bash
node index.js
```

Use Postman or curl to test the chat route:

```bash
curl -X POST http://localhost:3001/chat \
     -H "Content-Type: application/json" \
     -d '{"message": "Hello, who are you?"}'
```

You should receive a response from your bot ğŸ§ ğŸ’¬

With this, your chatbot backend is fully functional. You now have:

- An Express API that routes messages
- LangChain hooked to OpenAI with memory
- A clean project foundation to expand on

Next, weâ€™ll look into improving context awareness by expanding how memory works in LangChain ğŸ§ 

## ğŸ§  Adding Memory with LangChain: Context Awareness

Most real conversations rely on memory. Without it, every new message would feel like talking to a goldfish ğŸŸ. In this section, weâ€™ll make our chatbot context-aware using **LangChainâ€™s memory modules**.

By the end of this section, your bot will:

- Understand follow-up questions
- Refer back to previous user messages
- Feel more natural and intelligent ğŸ¤–

### ğŸ” What is BufferMemory?

`BufferMemory` is one of LangChainâ€™s simplest memory classes. It keeps a running history of the chat in memory. This is suitable for most short-lived chatbot sessions.

It stores conversation as a list of alternating messages (user â†’ bot â†’ user â†’ bot...) and passes the entire history into the prompt every time. No database needed.

### ğŸ§© Step 1: Refactor the LangChain Setup

Letâ€™s update `server/llm/langchain.js` to make memory a bit more dynamic.

```js
// server/llm/langchain.js
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationChain } from "langchain/chains";
import { BufferMemory } from "langchain/memory";

// ğŸ§  Create memory per session/user
const memoryStore = new Map();

function getChain(sessionId) {
  if (!memoryStore.has(sessionId)) {
    const memory = new BufferMemory();
    const model = new ChatOpenAI({
      temperature: 0.7,
      openAIApiKey: process.env.OPENAI_API_KEY,
    });
    const chain = new ConversationChain({ llm: model, memory });
    memoryStore.set(sessionId, chain);
  }
  return memoryStore.get(sessionId);
}

export async function runChat(input, sessionId = "default") {
  const chain = getChain(sessionId);
  const result = await chain.call({ input });
  return result.response;
}
```

**ğŸ§  Explanation:**

- We use a simple `Map` object to track memory by `sessionId`
- Each session gets its own `ConversationChain` and `BufferMemory`
- You can eventually replace this with Redis or DB-backed memory for production

### ğŸ› ï¸ Step 2: Update the Chat Route to Use Sessions

Update `server/routes/chat.js` to accept a session ID:

```js
// server/routes/chat.js
import express from "express";
import { runChat } from "../llm/langchain.js";

const router = express.Router();

router.post("/", async (req, res) => {
  const { message, sessionId } = req.body;
  if (!message) {
    return res.status(400).json({ error: "Message is required" });
  }
  try {
    const response = await runChat(message, sessionId);
    res.json({ response });
  } catch (err) {
    console.error("Chat error:", err);
    res.status(500).json({ error: "Something went wrong" });
  }
});

export default router;
```

Now, the API can track multiple conversations at once by passing different `sessionId`s.

You can test it like this:

```bash
curl -X POST http://localhost:3001/chat \
     -H "Content-Type: application/json" \
     -d '{"message": "Hello, who are you?", "sessionId": "abc123"}'
```

### ğŸ” Debugging Memory State

Want to peek under the hood? Log the memory content by modifying `runChat()`:

```js
console.log("\n--- Chat History ---\n", memory.chatHistory);
```

This will help you debug and understand how memory evolves.

### âœ… Result: Multi-Turn Chatbot

Youâ€™ve now built a chatbot that:

- Tracks conversation per session
- Supports multiple concurrent users
- Handles context like a pro

In the next section, weâ€™ll switch gears and build a beautiful **frontend interface in React** to talk to our bot ğŸ’¬

## ğŸ’¬ Building the React Frontend Chat Interface

With the backend running smoothly, itâ€™s time to give our chatbot a face! In this section, weâ€™ll build a sleek and responsive **React + TailwindCSS** frontend that connects to our Express + LangChain backend.

By the end, youâ€™ll have a fully functional chat UI with:

- Chat history
- Message input box
- Bot typing indicator
- API integration

### ğŸ§© Step 1: Create React App with Vite

Navigate to the project root and scaffold your frontend:

```bash
cd ..
npm create vite@latest client -- --template react
cd client
npm install
```

Then install Tailwind CSS:

```bash
npm install -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

Update `tailwind.config.js`:

```js
/** @type {import('tailwindcss').Config} */
export default {
  content: ["./index.html", "./src/**/*.{js,ts,jsx,tsx}"],
  theme: {
    extend: {},
  },
  plugins: [],
};
```

In `index.css`, replace with:

```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

### ğŸ› ï¸ Step 2: Build the Chat UI

Create a clean layout in `src/App.jsx`:

```jsx
import { useState } from "react";

function App() {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState("");
  const [loading, setLoading] = useState(false);
  const sessionId = "demo-session";

  const sendMessage = async () => {
    if (!input.trim()) return;
    const userMessage = { sender: "user", text: input };
    setMessages([...messages, userMessage]);
    setInput("");
    setLoading(true);

    try {
      const res = await fetch("http://localhost:3001/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ message: input, sessionId }),
      });
      const data = await res.json();
      const botMessage = { sender: "bot", text: data.response };
      setMessages((prev) => [...prev, botMessage]);
    } catch (err) {
      alert("Something went wrong");
    } finally {
      setLoading(false);
    }
  };

  const handleKeyPress = (e) => {
    if (e.key === "Enter") sendMessage();
  };

  return (
    <div className="min-h-screen bg-gray-100 flex flex-col items-center justify-center p-4">
      <div className="w-full max-w-xl bg-white rounded-xl shadow p-4 space-y-4">
        <div className="overflow-y-auto h-96 border p-3 rounded">
          {messages.map((msg, i) => (
            <div
              key={i}
              className={`mb-2 ${
                msg.sender === "user" ? "text-right" : "text-left"
              }`}
            >
              <span
                className={`inline-block px-3 py-2 rounded-lg ${
                  msg.sender === "user" ? "bg-blue-200" : "bg-gray-200"
                }`}
              >
                {msg.text}
              </span>
            </div>
          ))}
          {loading && (
            <div className="text-sm italic text-gray-500">Bot is typing...</div>
          )}
        </div>
        <div className="flex items-center gap-2">
          <input
            className="flex-1 border rounded px-3 py-2"
            placeholder="Type a message..."
            value={input}
            onChange={(e) => setInput(e.target.value)}
            onKeyDown={handleKeyPress}
          />
          <button
            onClick={sendMessage}
            className="bg-blue-500 text-white px-4 py-2 rounded"
          >
            Send
          </button>
        </div>
      </div>
    </div>
  );
}

export default App;
```

### ğŸ”Œ Step 3: Connect to Backend API

Make sure your backend (`localhost:3001`) is running when you test this. We use the `fetch()` API to send the message and receive a response.

**ğŸ’¡ Dev tip**: You can use Vite proxy for CORS-free development by updating `vite.config.js` later.

### âœ… Result: Interactive Chat UI

You now have a frontend that:

- Tracks messages in React state
- Sends input to your chatbot API
- Displays both user and bot replies
- Shows loading feedback

In the next part, weâ€™ll deploy this using Vercel and Railway so others can use your chatbot online ğŸš€

## ğŸš€ Free Deployment: Vercel (Frontend) + Railway (Backend)

Now that your chatbot works locally, letâ€™s take it live â€” for free! Weâ€™ll use **Vercel** to host the frontend and **Railway** to host the backend. Both have generous free tiers and integrate smoothly with GitHub.

Youâ€™ll end up with:

- A live website for your chatbot UI (Vercel)
- A live API server that handles LangChain + OpenAI logic (Railway)

### ğŸ§© Step 1: Push Your Code to GitHub

First, commit and push your project to a GitHub repo:

```bash
git init
git add .
git commit -m "Initial chatbot app"
git remote add origin https://github.com/yourusername/chatbot-openai-langchain-starter.git
git push -u origin main
```

### ğŸ› ï¸ Step 2: Deploy Backend to Railway

1. Go to [https://railway.app](https://railway.app)
2. Click **New Project â†’ Deploy from GitHub Repo**
3. Select your repo and choose the `server/` directory as root

#### ğŸ“ Railway Settings

- **Root Directory**: `server`
- **Start Command**: `node index.js`

#### ğŸ” Add Environment Variables

- Key: `OPENAI_API_KEY`
- Value: your OpenAI key

Railway will detect Express and auto-deploy it. After deployment, youâ€™ll get a public URL like:

```plaintext
https://chatbot-api-production.up.railway.app
```

### ğŸ¨ Step 3: Deploy Frontend to Vercel

1. Go to [https://vercel.com](https://vercel.com)
2. Click **Add New Project** and select your GitHub repo
3. Choose the `client/` folder as the project root

#### ğŸ›  Vercel Build Settings

- **Framework Preset**: Vite
- **Output Directory**: `dist`

#### ğŸ”Œ Add Environment Variable (Optional)

If youâ€™re proxying API URLs, you can set:

- `VITE_API_URL=https://your-backend-url/chat`

### âš™ï¸ Optional: Fix CORS on Backend

If your frontend canâ€™t reach your backend, you might need to tweak CORS:

```js
// server/index.js
app.use(cors({ origin: "https://your-frontend.vercel.app" }));
```

Or allow all (for testing only):

```js
app.use(cors());
```

### ğŸŒ Result: Public Chatbot App

You now have:

- Live frontend at `https://your-chatbot.vercel.app`
- Live backend API at `https://your-chatbot-api.railway.app`

Test it in the browser, share with friends, or embed it in your portfolio!

Next up: weâ€™ll add optional file upload + document Q&A using Retrieval-Augmented Generation ğŸ“„ğŸ§ 

## ğŸ“„ Optional: Add File Upload + RAG (Retrieval-Augmented Generation)

Want your chatbot to answer questions based on a **user-uploaded PDF**? This is where **RAG (Retrieval-Augmented Generation)** comes in.

Weâ€™ll enable:

- Uploading a PDF
- Parsing its text
- Embedding with OpenAI or Hugging Face
- Storing + searching in a local Chroma vector store
- Querying the doc contextually

All of this works on a **local, free setup** â€” no paid vector DBs needed! ğŸ§ 

### ğŸ“¦ Step 1: Install Additional Dependencies

In your `server/` folder:

```bash
npm install multer pdf-parse
```

These packages handle file uploads and PDF parsing.

### ğŸ“ Step 2: Add Upload Endpoint

Create `server/routes/upload.js`:

```js
import express from "express";
import multer from "multer";
import pdf from "pdf-parse";
import fs from "fs";
import path from "path";

const upload = multer({ dest: "uploads/" });
const router = express.Router();

router.post("/", upload.single("file"), async (req, res) => {
  const filePath = req.file.path;
  try {
    const dataBuffer = fs.readFileSync(filePath);
    const parsed = await pdf(dataBuffer);
    fs.unlinkSync(filePath); // delete uploaded file

    res.json({ text: parsed.text });
  } catch (err) {
    console.error("PDF parse error:", err);
    res.status(500).json({ error: "Failed to parse PDF" });
  }
});

export default router;
```

In `index.js`:

```js
import uploadRoute from "./routes/upload.js";
app.use("/upload", uploadRoute);
```

### ğŸ§  Step 3: Create Vector Store + QA Chain

Create `server/llm/retriever.js`:

```js
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { Chroma } from "langchain/vectorstores/chroma";
import { RetrievalQAChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { TextLoader } from "langchain/document_loaders/fs/text";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import fs from "fs";

let vectorStore;

export async function createKnowledgeBase(text) {
  fs.writeFileSync("uploads/docs.txt", text);

  const loader = new TextLoader("uploads/docs.txt");
  const docs = await loader.load();

  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200,
  });
  const chunks = await splitter.splitDocuments(docs);

  const embeddings = new OpenAIEmbeddings();
  vectorStore = await Chroma.fromDocuments(chunks, embeddings);
}

export async function askDocQuestion(query) {
  if (!vectorStore) throw new Error("No knowledge base loaded");
  const model = new ChatOpenAI();
  const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever());
  const result = await chain.call({ query });
  return result.text;
}
```

### ğŸ”Œ Step 4: Create Chat Route for RAG

In `server/routes/chat-doc.js`:

```js
import express from "express";
import { askDocQuestion } from "../llm/retriever.js";

const router = express.Router();

router.post("/", async (req, res) => {
  const { message } = req.body;
  if (!message) return res.status(400).json({ error: "Message required" });
  try {
    const response = await askDocQuestion(message);
    res.json({ response });
  } catch (err) {
    console.error("RAG chat error:", err);
    res.status(500).json({ error: "RAG failed" });
  }
});

export default router;
```

Add this to `index.js`:

```js
import chatDocRoute from "./routes/chat-doc.js";
app.use("/chat-doc", chatDocRoute);
```

### âœ… Final Flow

1. Upload PDF â†’ Extract + embed text
2. Store in Chroma (in-memory vector DB)
3. Ask questions â†’ Retrieve relevant chunks â†’ LLM answers based on them

Next, weâ€™ll polish UX with reset, typing indicators, and error boundaries ğŸ’¬âœ¨

## âœ¨ UX Polish: Typing Indicators, Reset Button, and Error Handling

Now that our chatbot is feature-rich, letâ€™s take care of the **user experience (UX)** â€” the part that makes it feel truly alive.

Weâ€™ll improve:

- âœ… Visual feedback ("Bot is typingâ€¦")
- ğŸ—‘ï¸ Reset chat option
- ğŸš¨ Error messages

These polish points might seem small, but they dramatically enhance usability and make your bot feel professional.

### ğŸ¨ Step 1: Add Typing Indicator

Update `src/App.jsx` to show a message while the bot is generating a response.

Hereâ€™s the modified React logic:

```jsx
// src/App.jsx
const [loading, setLoading] = useState(false);

...

{messages.map((msg, i) => (
  <div key={i} className={`mb-2 ${msg.sender === 'user' ? 'text-right' : 'text-left'}`}>
    <span className={`inline-block px-3 py-2 rounded-lg ${msg.sender === 'user' ? 'bg-blue-200' : 'bg-gray-200'}`}>
      {msg.text}
    </span>
  </div>
))}
{loading && <div className="text-left text-sm italic text-gray-500">Bot is typing...</div>}
```

Now every time you send a message, the UI gives immediate feedback that somethingâ€™s happening.

### ğŸ”„ Step 2: Add Reset Button

Letâ€™s allow users to clear the chat history:

```jsx
// Inside return()
<button
  onClick={() => setMessages([])}
  className="text-sm text-red-500 underline ml-auto block mt-1"
>
  Clear Chat
</button>
```

Add this just below the chat history container. It simply resets the `messages` array.

### ğŸ§¯ Step 3: Add Error Handling

Wrap your `fetch()` call in a `try/catch`, and show alerts or inline error messages:

```jsx
try {
  const res = await fetch("http://localhost:3001/chat", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ message: input, sessionId }),
  });
  if (!res.ok) throw new Error("API failed");
  const data = await res.json();
  const botMessage = { sender: "bot", text: data.response };
  setMessages((prev) => [...prev, botMessage]);
} catch (err) {
  console.error(err);
  setMessages((prev) => [
    ...prev,
    { sender: "bot", text: "âš ï¸ Something went wrong. Please try again." },
  ]);
}
```

This way, even if something breaks, the user isnâ€™t left staring at a frozen screen.

### ğŸ’„ Result: A Smooth, Polished UX

With these enhancements:

- The bot feels responsive
- Users can reset the chat any time
- Errors are caught and communicated clearly

In the final article sections, weâ€™ll look at **prompt engineering**, **LangChain debugging**, and wrap up the repo and deployment âœ¨

## ğŸ§ª Bonus Tips: LangChain Debugging + Prompt Engineering

To truly master chatbot development, you need two extra skills:

- ğŸ›  **Debugging LangChain chains**
- ğŸ¨ **Crafting effective prompts**

These skills can help you build more reliable, intelligent, and tailored bots â€” especially when handling tricky customer use-cases.

### ğŸ§­ Step 1: Enable LangChain Debug Mode

LangChain can print out internal chain activity using a built-in global flag.

Add this to your entry point (e.g. `index.js`):

```js
process.env.LANGCHAIN_HANDLER = "console";
```

Alternatively, in code:

```js
import { setHandler } from "langchain/callbacks";
setHandler("console");
```

Now when you run your bot, youâ€™ll see:

- Prompts being generated
- Token counts
- LLM call/response timings

Super useful for tracing what went wrong or tuning performance.

### âœï¸ Step 2: Use Prompt Templates

LangChain lets you define reusable, fill-in-the-blank prompts.

Hereâ€™s how you could rewrite `langchain.js` using a custom system prompt:

```js
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { BufferMemory } from "langchain/memory";

const prompt = PromptTemplate.fromTemplate(`
You are a helpful website assistant. Always answer clearly.

Conversation so far:
{history}

User: {input}
Assistant:
`);

const memory = new BufferMemory();
const llm = new ChatOpenAI({
  temperature: 0.7,
  openAIApiKey: process.env.OPENAI_API_KEY,
});

const chain = new LLMChain({ prompt, llm, memory });

export async function runChat(input) {
  const result = await chain.call({ input });
  return result.text;
}
```

This gives you much more control over:

- Personality
- Format
- Tone

### ğŸ‘¤ Step 3: Try Persona Prompts

Make your assistant adopt a role, like:

```text
You are a sarcastic AI assistant who gives witty answers.
You remember the user's favorite color and always mention it.
```

Or for serious use cases:

```text
You are a financial assistant who gives concise and factual answers.
If unsure, say "I don't know".
```

Use `PromptTemplate.fromTemplate(...)` to inject these styles into your chain.

### ğŸ§  Step 4: Mix with Memory

When using `BufferMemory`, LangChain automatically fills `{history}` with past turns. So your prompt templates can smoothly incorporate context-aware behavior.

If you want more control, you can manually access memory too:

```js
console.log(await memory.loadMemoryVariables());
```

### âœ… Takeaway

With debugging and prompt design:

- You can spot problems faster
- Customize behavior precisely
- Optimize tokens and cost

In the final section, weâ€™ll wrap up the project and share the GitHub repo ğŸ‰

## ğŸ“¦ Final Thoughts + GitHub Repo Setup

Youâ€™ve now built a **full-stack, context-aware, deployable chatbot** with LangChain and OpenAI â€” and best of all, using only **free-tier tools**. Letâ€™s wrap up the series by reviewing what youâ€™ve built, finalizing the GitHub repo, and suggesting ways to expand your bot further.

### ğŸ§± What Youâ€™ve Built

âœ… Backend API (Express + LangChain)

- Session-based memory with BufferMemory
- Optional RAG (Retrieval-Augmented Generation) with Chroma
- Robust routing with error handling

âœ… Frontend UI (React + Tailwind)

- Live chat interface with typing indicators
- Reset and error UX
- API integration with session-based context

âœ… Deployment

- Backend live on Railway
- Frontend live on Vercel
- Connected via environment config

âœ… Optional Features

- Upload PDF â†’ Embed â†’ Ask questions
- Prompt engineering + persona styling

### ğŸ“ Recommended Repo Structure

If you havenâ€™t already, organize your files like this:

```plaintext
chatbot-openai-langchain-starter/
â”œâ”€â”€ client/                  # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ server/                 # Express + LangChain backend
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ uploads/            # Optional file-based RAG
â”‚   â””â”€â”€ index.js
â”œâ”€â”€ .env.example
â”œâ”€â”€ README.md
```

Include a `.env.example` like:

```env
OPENAI_API_KEY=your_openai_key_here
```

### ğŸ“ Sample README.md Template

Create a friendly README for devs exploring your bot:

````md
# ğŸ¤– Chatbot Starter (LangChain + OpenAI)

A full-stack chatbot using React + Express + LangChain. Context-aware, memory-enabled, and free-tier deployable.

## ğŸ§  Features

- Chat with OpenAI (GPT-3.5)
- Context memory per session
- Optional: Ask questions on uploaded PDFs
- Free deployment (Vercel + Railway)

## ğŸš€ Quick Start

```bash
git clone https://github.com/yourusername/chatbot-openai-langchain-starter
cd server && npm install && node index.js
cd ../client && npm install && npm run dev
```

## ğŸ” Env Variables

- `OPENAI_API_KEY` (backend)
- `VITE_API_URL` (optional, frontend)

## ğŸ“¦ Built With

- LangChain JS
- OpenAI
- Express
- React + TailwindCSS
- Railway + Vercel
````

### ğŸŒ± Ideas for Extensions

Here are a few ways to take this bot to the next level:

- ğŸ§‘â€ğŸ’» Add authentication with Supabase or Clerk
- ğŸ—£ Use Whisper for voice input
- ğŸŒ Make it multilingual with Google Translate API
- ğŸ§  Switch to Hugging Face models for self-hosting
- ğŸ“œ Save chat history to a DB (e.g., SQLite or Supabase)

### ğŸ“Œ Final Repo Suggestion

Name: `chatbot-openai-langchain-starter`

Include a clean `README.md`, environment sample, and clear folder structure. You can even deploy a live demo and link it in your GitHub profile.

Congrats ğŸ‰ â€” Youâ€™ve not just followed a tutorial, youâ€™ve built a deployable, extensible chatbot foundation you can fork, scale, and customize as needed!

See you in the next series â€” where weâ€™ll dive deeper into **advanced memory**, **agent routing**, and **multi-modal input** ğŸ‘‹

---

**Hey, Iâ€™m Darshan Jitendra Chobarkar** â€” a freelance full-stack web developer surviving the caffeinated chaos of coding from Pune â˜•ğŸ’» If you enjoyed this article (or even skimmed through while silently judging my code), you might like the rest of my tech adventures.

ğŸ”— Explore more writeups, walkthroughs, and side projects at [dchobarkar.github.io](https://dchobarkar.github.io/)  
ğŸ” Curious where the debugging magic happens? Check out my commits at [github.com/dchobarkar](https://github.com/dchobarkar)  
ğŸ‘” Letâ€™s connect professionally on [LinkedIn](https://www.linkedin.com/in/dchobarkar/)

Thanks for reading â€” and if youâ€™ve got thoughts, questions, or feedback, Iâ€™d genuinely love to hear from you. This blogâ€™s not just a portfolio â€” itâ€™s a conversation. Letâ€™s keep it going ğŸ‘‹
