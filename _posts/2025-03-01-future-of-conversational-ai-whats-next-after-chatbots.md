# Conversational AI: Building Intelligent Chatbots Across Platforms - 10: Future of Conversational AI: What‚Äôs Next After Chatbots?

## üöÄ Rethinking the Chatbot: From Text to Multi-Modal Agents

For years, chatbots have been the face of conversational AI. They popped up on landing pages, helped users navigate e-commerce sites, and handled basic customer support. Most were rule-based or had limited NLP capabilities. But as we step into 2025, the question isn't _"How do we make a better chatbot?"_ ‚Äî it's \*"What comes after chatbots?"

The answer lies in **multi-modal AI agents** ‚Äî intelligent systems that go beyond just text, able to understand images, parse voice input, interpret documents, and take autonomous actions. Let's unpack why this shift is happening and what it means for developers like us.

### üß† Why Chatbots Are Just the Beginning

Traditional chatbots are essentially glorified state machines:

- They operate within pre-defined rules
- Often rely on keyword matching or rigid NLP
- Lack long-term memory or contextual awareness

Even with GPT-powered chatbots, many are stuck in a **"single-modality loop"** ‚Äî they only understand and respond to text.

But users are inherently **multi-modal communicators**:

- We speak, type, point, draw, and upload
- We expect systems to _see_, _hear_, and _respond_ accordingly

Large Language Models (LLMs) like **GPT-4o**, **Gemini 1.5**, and **Claude 3** now support these capabilities, making it possible to build assistants that can:

- üëÅ Interpret images and screenshots
- üé§ Listen to voice queries and respond with speech
- üìù Read documents and extract data
- ü§ñ Take actions (click buttons, run code, call APIs)

These aren‚Äôt just chatbots anymore. They're **autonomous agents**.

### üåç Real-World Examples of Next-Gen Assistants

We‚Äôre already seeing products push these boundaries:

- **Humane AI Pin**: A wearable voice-first AI with projector UI
- **Rabbit R1**: A pocket-sized agent trained to use other apps
- **GPT-4o with Voice Mode**: Real-time conversation with personality, memory, and emotional tone
- **Meta AI on Ray-Bans**: Visual and conversational assistant that works on-the-go

All of them share a vision: **an AI assistant that is always-on, context-aware, and multi-modal.**

### üìä Why This Matters for Developers

As developers, this opens new frontiers:

- **UI/UX** shifts from static chat boxes to dynamic, voice/image-enabled interfaces
- **Architectures** now include audio pipelines, vision models, and agent-based reasoning
- **Tooling** expands: Whisper for transcription, LangChain Agents for autonomy, Vercel Edge for responsiveness

> We‚Äôre no longer building "bots". We're building _assistants_, _co-pilots_, and even _collaborators_.

And the best part? The APIs and SDKs are ready. You can build this today.

Starting in the next section, we'll walk through a complete hands-on project where you build your own multi-modal AI agent using:

- **Next.js** (frontend)
- **OpenAI (GPT-4o + Whisper)**
- **LangChain** (tooling + agent reasoning)
- **Vercel** (deployment)

Let's dive in ‚ú®

## üß† What Makes a ‚ÄúNext-Gen‚Äù Assistant?

To build what comes _after_ chatbots, we first need to understand what makes a "next-generation AI assistant" truly different. These assistants aren't just better chat interfaces. They are fundamentally more capable, more integrated, and more autonomous.

Let's break it down.

### 1. üîç Perception: Multi-Modal Input Handling

**Text-only input is no longer enough.** A next-gen assistant can ingest and process:

- üìÑ Text (obviously)
- üì∏ Images and screenshots
- üé§ Voice and audio input
- üìé File uploads (PDFs, CSVs, docs)

> These assistants "see" your screen, "hear" your voice, and "read" your docs.

**Why it matters:** This enables the assistant to work in real-world contexts: summarizing meeting recordings, analyzing screenshots, transcribing voice memos, or reading contracts.

### 2. üèõÔ∏è Reasoning: Beyond Intents, Into Autonomy

Traditional chatbots are stuck in the intent-slot-response loop. Assistants break free by using **agentic reasoning**:

- They break tasks into sub-tasks
- They decide _what_ to do and _which tool_ to use
- They learn and adapt based on past interactions

This is made possible by tools like **LangChain Agents**, **AutoGPT**, and **CrewAI**.

Example pseudo-flow:

```txt
User: "Summarize the key metrics from this PDF and send me a voice note."
Agent:
- Load PDF ‚úâ
- Extract tables üìä
- Generate summary ‚úèÔ∏è
- Convert to speech üé§
- Send audio file ‚úâ
```

This is task execution, not just chat.

### 3. üìä Memory: Context Persistence and Personalization

Good assistants **remember**:

- Past conversations
- User preferences
- Task history and status

There are two types:

- **Short-term memory**: Conversation-level (LangChain Memory)
- **Long-term memory**: Persistent (via Supabase, Redis, or vector DBs)

```ts
// Example: LangChain memory config
const memory = new BufferMemory({
  returnMessages: true,
  memoryKey: "chat_history",
});
```

With memory, your agent can say:

> "Last week you mentioned migrating to PostgreSQL. Want me to generate that schema again?"

### 4. üîß Tooling: Calling External APIs and Systems

These assistants aren't islands. They use tools like:

- Web search
- File I/O
- Databases
- Code execution (Python, JS)
- External APIs (weather, news, calendar)

LangChain provides a powerful abstraction:

```ts
const tools = [calculator, search, fileReader];
const agent = initializeAgentExecutorWithOptions(tools, model, {
  agentType: "openai-functions",
});
```

With tools, the assistant becomes actionable ‚Äî not just conversational.

### 5. ‚è± Real-Time Interaction: Instant Feedback and Streaming

Nobody wants to wait 10 seconds for a response. Assistants now support:

- **Streaming outputs** (using OpenAI's `stream: true`)
- **Voice synthesis** (text-to-speech)
- **Real-time typing indicators**

Example: OpenAI streaming

```ts
const response = await openai.chat.completions.create({
  model: "gpt-4o",
  stream: true,
  messages,
});
```

### 6. üé® Persona: Identity, Tone, and Emotional Intelligence

Assistants now reflect _personality_ and _tone_ ‚Äî serious, casual, witty, empathetic.

You can set this via system prompts:

```ts
const messages = [
  {
    role: "system",
    content: "You are Ava, a helpful but witty AI research assistant.",
  },
];
```

Some models like GPT-4o even modulate **voice tone** to sound surprised, thoughtful, or curious. This makes assistants more human-like and engaging.

### üöÄ Summary: From Bots to AI Colleagues

A next-gen assistant:

- Accepts text, images, voice, and files
- Remembers and adapts
- Uses tools and APIs
- Responds in real-time
- Shows personality and tone

We‚Äôre building not just chat interfaces, but digital colleagues that _see, hear, understand, act,_ and _improve over time._

In the next section, we'll begin our hands-on journey to build such an assistant from scratch ‚Äî starting with setting up the tech stack. Let's go üöÄ

## ‚öôÔ∏è Project Setup: Building a Multi-Modal AI Agent

Before we dive into building the multi-modal assistant, let's get our foundation solid. This section will walk you through the full stack setup ‚Äî folder structure, tools, packages, API keys, and configurations ‚Äî so you're ready to build.

We'll be building a **Next.js 14 app** powered by:

- **GPT-4o** for multi-modal reasoning
- **Whisper API** for speech-to-text
- **LangChain** for agentic execution
- **Vercel** for deployment
- **TailwindCSS** for frontend styling

### üîß GitHub Repo Name Suggestion

```txt
multi-modal-ai-agent-nextjs
```

You can organize it as a mono-repo if needed, but a single Next.js app will suffice.

### üìÇ Suggested Folder Structure

```bash
multi-modal-ai-agent-nextjs/
‚îú‚îÄ‚îÄ app/                     # Next.js pages + routing
‚îú‚îÄ‚îÄ components/             # React UI components
‚îú‚îÄ‚îÄ lib/                    # Core logic: OpenAI, LangChain, Whisper
‚îú‚îÄ‚îÄ public/                 # Static files (icons, loaders)
‚îú‚îÄ‚îÄ styles/                 # Tailwind configs
‚îú‚îÄ‚îÄ .env.local              # Environment variables
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ next.config.js
‚îî‚îÄ‚îÄ README.md
```

### üìÉ Required Environment Variables

Create a `.env.local` file at the root:

```env
OPENAI_API_KEY=sk-xxxx
NEXT_PUBLIC_OPENAI_MODEL=gpt-4o
WHISPER_API_KEY=sk-xxxx
```

Later, you can add vector DB keys (like Supabase or Pinecone) for memory.

### üìö Install Dependencies

Use `pnpm` or `npm`, your choice:

```bash
pnpm init next-app multi-modal-ai-agent-nextjs --ts
cd multi-modal-ai-agent-nextjs
pnpm add openai langchain axios react-icons formik
pnpm add -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

Also install OpenAI SDK:

```bash
pnpm add openai@latest
```

If you're using Whisper or ElevenLabs for voice, add:

```bash
pnpm add form-data multer
```

### üîç Tailwind Configuration

In `tailwind.config.js`:

```js
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: ["./app/**/*.{js,ts,jsx,tsx}", "./components/**/*.{js,ts,jsx,tsx}"],
  theme: {
    extend: {},
  },
  plugins: [],
};
```

In `styles/globals.css`:

```css
@tailwind base;
@tailwind components;
@tailwind utilities;
```

### üîí OpenAI Initialization

Create `lib/openai.ts`:

```ts
import OpenAI from "openai";

export const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || "",
});
```

Same for LangChain:

```ts
import { ChatOpenAI } from "langchain/chat_models/openai";

export const chatModel = new ChatOpenAI({
  temperature: 0.7,
  modelName: process.env.NEXT_PUBLIC_OPENAI_MODEL,
});
```

### üìº Optional: Whisper API Proxy

If you want to record voice and transcribe via Whisper, you'll need a server route:

In `app/api/transcribe/route.ts`:

```ts
import { NextRequest, NextResponse } from "next/server";
import formidable from "formidable";
import fs from "fs";

export const config = {
  api: {
    bodyParser: false,
  },
};

export async function POST(req: NextRequest) {
  const form = formidable({ multiples: false });
  const [fields, files] = await new Promise((resolve, reject) => {
    form.parse(req, (err, fields, files) => {
      if (err) reject(err);
      else resolve([fields, files]);
    });
  });

  const file = fs.createReadStream(files.audio.filepath);
  const res = await fetch("https://api.openai.com/v1/audio/transcriptions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${process.env.WHISPER_API_KEY}`,
    },
    body: file,
  });

  const data = await res.json();
  return NextResponse.json(data);
}
```

### üöÄ You‚Äôre Ready to Build

At this point, you should have:

- A fully bootstrapped Next.js + Tailwind + OpenAI project
- Basic scaffolding for LangChain and Whisper
- API keys set up in your `.env`

In the next section, we'll begin with the **text chat interface** using GPT-4o and LangChain memory ‚Äî and gradually build toward full multi-modality. Let's go ‚ú®

## üí¨ Step 1: Text Chat Interface (GPT-4o / gpt-4-turbo)

Let‚Äôs begin our multi-modal assistant by building the **core text chat interface** ‚Äî the foundation for everything else. We‚Äôll use:

- **GPT-4o** (or `gpt-4-turbo`) as the backend model
- **LangChain** for memory and message formatting
- **Next.js API Routes** for server logic
- **TailwindCSS** for UI

### üåê Frontend Chat UI (React + Tailwind)

In `components/ChatBox.tsx`:

```tsx
"use client";
import { useState } from "react";

export default function ChatBox() {
  const [messages, setMessages] = useState<{ role: string; content: string }[]>(
    []
  );
  const [input, setInput] = useState("");

  async function sendMessage() {
    const userMessage = { role: "user", content: input };
    setMessages([...messages, userMessage]);
    setInput("");

    const res = await fetch("/api/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ messages: [...messages, userMessage] }),
    });

    const data = await res.json();
    setMessages([...messages, userMessage, data.reply]);
  }

  return (
    <div className="max-w-xl mx-auto p-4">
      <div className="space-y-2 mb-4">
        {messages.map((msg, i) => (
          <div
            key={i}
            className={`p-2 rounded-lg ${
              msg.role === "user" ? "bg-blue-100" : "bg-gray-100"
            }`}
          >
            <strong>{msg.role}:</strong> {msg.content}
          </div>
        ))}
      </div>
      <input
        className="border p-2 w-full mb-2"
        value={input}
        onChange={(e) => setInput(e.target.value)}
        placeholder="Ask something..."
      />
      <button
        onClick={sendMessage}
        className="bg-blue-500 text-white px-4 py-2 rounded"
      >
        Send
      </button>
    </div>
  );
}
```

Then in your app's homepage `app/page.tsx`:

```tsx
import ChatBox from "@/components/ChatBox";
export default function Home() {
  return <ChatBox />;
}
```

### üîß Backend API Route (LangChain + GPT-4o)

In `app/api/chat/route.ts`:

```ts
import { ChatOpenAI } from "langchain/chat_models/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";
import { OpenAIStream, StreamingTextResponse } from "ai";

const model = new ChatOpenAI({
  temperature: 0.7,
  streaming: true,
  modelName: process.env.NEXT_PUBLIC_OPENAI_MODEL || "gpt-4o",
});

const memory = new BufferMemory({
  returnMessages: true,
  memoryKey: "chat_history",
});

const chain = new ConversationChain({ llm: model, memory });

export async function POST(req: Request) {
  const { messages } = await req.json();
  const res = await chain.call({
    input: messages[messages.length - 1].content,
  });
  return Response.json({ reply: { role: "assistant", content: res.response } });
}
```

### üîä Bonus: Enable Streaming Response

To stream tokens from OpenAI:

1. Use OpenAI's `stream: true`
2. Integrate with `ai` SDK (or custom logic)

Install:

```bash
pnpm add ai
```

Update `route.ts` for streaming:

```ts
export async function POST(req: Request) {
  const { messages } = await req.json();
  const response = await model.call({ messages });
  const stream = OpenAIStream(response);
  return new StreamingTextResponse(stream);
}
```

Or handle streaming manually by chunking the response.

### üöÄ You Have a Working Chat Interface

You can now:

- Ask questions
- Get contextual replies
- Maintain memory via LangChain

This is the conversational backbone for our assistant. Next, we‚Äôll expand it to handle image input with GPT-4o Vision capabilities. Stay tuned! üöÄ

## üñºÔ∏è Step 2: Image Input + Vision Response

Now that we have our chat interface, it‚Äôs time to level it up: **let your assistant see**. We'll enable users to upload images or screenshots, and use **GPT-4o** (or GPT-4 with vision) to analyze and respond to them contextually.

This is where multi-modality gets real. Users can:

- Upload receipts or bills for analysis
- Submit screenshots for debugging
- Share photos or diagrams for interpretation

### üìÅ Update Folder Structure

Add a new folder:

```bash
components/ImageChatBox.tsx
```

And optionally a subfolder:

```bash
public/uploads
```

### üåê Frontend: Image + Text Upload Form

In `components/ImageChatBox.tsx`:

```tsx
"use client";
import { useState } from "react";

export default function ImageChatBox() {
  const [messages, setMessages] = useState<any[]>([]);
  const [image, setImage] = useState<File | null>(null);
  const [input, setInput] = useState("");

  const handleImageChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    if (e.target.files) setImage(e.target.files[0]);
  };

  async function sendMessage() {
    const formData = new FormData();
    formData.append("image", image as Blob);
    formData.append("prompt", input);

    const res = await fetch("/api/vision", {
      method: "POST",
      body: formData,
    });
    const data = await res.json();

    setMessages([
      ...messages,
      { role: "user", content: input },
      { role: "assistant", content: data.reply },
    ]);
    setInput("");
    setImage(null);
  }

  return (
    <div className="max-w-xl mx-auto p-4">
      <div className="space-y-2 mb-4">
        {messages.map((msg, i) => (
          <div
            key={i}
            className={`p-2 rounded-lg ${
              msg.role === "user" ? "bg-blue-100" : "bg-gray-100"
            }`}
          >
            <strong>{msg.role}:</strong> {msg.content}
          </div>
        ))}
      </div>
      <input
        type="file"
        accept="image/*"
        onChange={handleImageChange}
        className="mb-2"
      />
      <input
        className="border p-2 w-full mb-2"
        value={input}
        onChange={(e) => setInput(e.target.value)}
        placeholder="Ask something about the image..."
      />
      <button
        onClick={sendMessage}
        className="bg-green-500 text-white px-4 py-2 rounded"
      >
        Submit
      </button>
    </div>
  );
}
```

Use it in a route/page like:

```tsx
import ImageChatBox from "@/components/ImageChatBox";
export default function VisionPage() {
  return <ImageChatBox />;
}
```

### üîß Backend: Vision Analysis via GPT-4o

In `app/api/vision/route.ts`:

```ts
import { NextRequest, NextResponse } from "next/server";
import { openai } from "@/lib/openai";
import { writeFile } from "fs/promises";
import path from "path";
import { randomUUID } from "crypto";

export async function POST(req: NextRequest) {
  const formData = await req.formData();
  const prompt = formData.get("prompt") as string;
  const file = formData.get("image") as File;

  const buffer = Buffer.from(await file.arrayBuffer());
  const filename = `${randomUUID()}.png`;
  const filepath = path.join(process.cwd(), "public/uploads", filename);

  await writeFile(filepath, buffer);
  const url = `${req.nextUrl.origin}/uploads/${filename}`;

  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      {
        role: "system",
        content:
          "You are a helpful assistant that can analyze images and screenshots.",
      },
      {
        role: "user",
        content: [
          { type: "text", text: prompt },
          { type: "image_url", image_url: { url } },
        ],
      },
    ],
  });

  const reply = response.choices[0].message.content;
  return NextResponse.json({ reply });
}
```

### üöÄ Use Cases & Examples

You can now:

- Upload a UI bug screenshot and ask: _"What's the likely issue?"_
- Share a chart and ask: _"Summarize key insights"_
- Upload a photo of handwritten notes and ask: _"Convert this to a list"_

This builds a more _observational_ AI system, not just a reactive one.

In the next step, we‚Äôll bring in **voice interaction** ‚Äî so your assistant can hear and speak too. Let‚Äôs go! üé§

## üé§ Step 3: Voice Interaction with Whisper + TTS

Now it‚Äôs time to bring sound into the picture ‚Äî literally. In this section, we‚Äôll build a full voice loop:

1. Record user audio in the browser
2. Transcribe audio to text using OpenAI Whisper API
3. Process it with GPT-4o (optional)
4. Convert assistant response to speech using TTS (Text-to-Speech)

We‚Äôre building a real voice assistant here ‚Äî not just a chatbot. Let‚Äôs go. üöÄ

### üåê Frontend: Record + Play + Send Audio

In `components/VoiceChatBox.tsx`:

```tsx
"use client";
import { useState, useRef } from "react";

export default function VoiceChatBox() {
  const [transcript, setTranscript] = useState("");
  const [reply, setReply] = useState("");
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunks: Blob[] = [];

  const startRecording = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const mediaRecorder = new MediaRecorder(stream);
    mediaRecorderRef.current = mediaRecorder;
    mediaRecorder.start();

    mediaRecorder.ondataavailable = (e) => audioChunks.push(e.data);
  };

  const stopRecording = async () => {
    const mediaRecorder = mediaRecorderRef.current;
    mediaRecorder?.stop();

    mediaRecorder!.onstop = async () => {
      const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
      const formData = new FormData();
      formData.append("audio", audioBlob);

      const res = await fetch("/api/voice", {
        method: "POST",
        body: formData,
      });
      const data = await res.json();
      setTranscript(data.transcript);
      setReply(data.reply);

      const audio = new Audio(data.audioUrl);
      audio.play();
    };
  };

  return (
    <div className="max-w-xl mx-auto p-4">
      <button
        onClick={startRecording}
        className="bg-red-500 text-white px-4 py-2 m-2 rounded"
      >
        Record
      </button>
      <button
        onClick={stopRecording}
        className="bg-green-500 text-white px-4 py-2 m-2 rounded"
      >
        Stop
      </button>
      <div className="mt-4">
        <p>
          <strong>You said:</strong> {transcript}
        </p>
        <p>
          <strong>Assistant:</strong> {reply}
        </p>
      </div>
    </div>
  );
}
```

Use it in a route like:

```tsx
import VoiceChatBox from "@/components/VoiceChatBox";
export default function VoicePage() {
  return <VoiceChatBox />;
}
```

### üîß Backend: Whisper + TTS

In `app/api/voice/route.ts`:

```ts
import { NextRequest, NextResponse } from "next/server";
import formidable from "formidable";
import fs from "fs";
import { openai } from "@/lib/openai";
import path from "path";
import { writeFile } from "fs/promises";

export const config = {
  api: { bodyParser: false },
};

export async function POST(req: NextRequest) {
  const form = formidable({ multiples: false });
  const [fields, files] = await new Promise((resolve, reject) => {
    form.parse(req, (err, fields, files) => {
      if (err) reject(err);
      else resolve([fields, files]);
    });
  });

  const audio = fs.createReadStream(files.audio.filepath);

  const transcriptRes = await openai.audio.transcriptions.create({
    file: audio,
    model: "whisper-1",
  });
  const transcript = transcriptRes.text;

  const completion = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: transcript },
    ],
  });

  const reply = completion.choices[0].message.content || "";

  // Generate audio via TTS (replace with ElevenLabs or browser TTS)
  const audioUrl = `https://api.streamelements.com/kappa/v2/speech?voice=Brian&text=${encodeURIComponent(
    reply
  )}`;

  return NextResponse.json({ transcript, reply, audioUrl });
}
```

### üöÄ Bonus: TTS with ElevenLabs

To use a more natural-sounding voice:

- Sign up at [https://www.elevenlabs.io/](https://www.elevenlabs.io/)
- Use their API to convert `reply` to audio

```ts
const audioRes = await fetch(
  "https://api.elevenlabs.io/v1/text-to-speech/{voice_id}",
  {
    method: "POST",
    headers: {
      "xi-api-key": process.env.ELEVENLABS_API_KEY,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({ text: reply, model_id: "eleven_monolingual_v1" }),
  }
);
```

Save the response and serve it to the frontend.

### üöÄ What You've Built

You now have:

- Real-time voice-to-text transcription (Whisper)
- AI-powered response (GPT-4o)
- Text-to-speech reply (TTS)

This unlocks use cases like:

- Voice-first mobile agents
- Conversational kiosks
- Assistive tech for accessibility

Next up: we‚Äôll integrate **LangChain tools** to let your assistant _do things_, not just talk ‚Äî like search, code, and analyze files.

## üõ†Ô∏è Step 4: Tool-Enabled Reasoning (LangChain Agents)

Now that your assistant can chat, see, and listen, it‚Äôs time to make it **act**. This section introduces **LangChain agents** and tools, enabling your assistant to:

- Search the web üîç
- Run calculations ‚úñÔ∏è
- Read files üìÑ
- Execute code üíª

Let‚Äôs give your assistant the power to reason and decide _how_ to respond.

### ü§ñ What Is an Agent?

In LangChain, an **agent** is a wrapper around an LLM that can:

- Dynamically choose from a list of tools
- Parse user intent
- Decide on intermediate actions
- Execute those actions, and respond

Think of it like an AI brain with arms.

### üîä Installing LangChain Tools

Make sure you have:

```bash
pnpm add langchain@latest
pnpm add @langchain/core
```

### üìä Step-by-Step: Setup Agent with Tools

In `lib/agent.ts`:

```ts
import { ChatOpenAI } from "langchain/chat_models/openai";
import { Calculator } from "langchain/tools/calculator";
import { SerpAPI } from "langchain/tools";
import { initializeAgentExecutorWithOptions } from "langchain/agents";

const llm = new ChatOpenAI({
  modelName: "gpt-4o",
  temperature: 0.7,
});

const tools = [new Calculator(), new SerpAPI(process.env.SERP_API_KEY!)];

export async function runAgent(input: string) {
  const executor = await initializeAgentExecutorWithOptions(tools, llm, {
    agentType: "openai-functions",
  });
  const result = await executor.run(input);
  return result;
}
```

### üîß Create API Route to Call Agent

In `app/api/agent/route.ts`:

```ts
import { runAgent } from "@/lib/agent";
import { NextRequest, NextResponse } from "next/server";

export async function POST(req: NextRequest) {
  const { input } = await req.json();
  const result = await runAgent(input);
  return NextResponse.json({ result });
}
```

### üîç Test Your Agent

In the browser or Postman:

```json
POST /api/agent
{
  "input": "What's the weather in London and what is 23 * 17?"
}
```

The agent should:

- Use SerpAPI to fetch weather
- Use Calculator to compute 23 \* 17
- Return a coherent response

### üöÄ Add to Chat Frontend (Optional)

In your chat component, add an `agentMode` toggle:

```ts
const useAgent = true;
const url = useAgent ? "/api/agent" : "/api/chat";
```

### üîÆ Add Custom Tools

You can define your own tools too. Example: reading a PDF file.

```ts
import { Tool } from "langchain/tools";

const fileReaderTool = new Tool({
  name: "read_file",
  description: "Reads uploaded files and extracts key content",
  func: async (text) => {
    // parse local file or buffer
    return "Parsed content...";
  },
});
```

Add it to your `tools` array and the agent will decide when to use it.

### üöÄ Agent Use Cases

With tools in place, your assistant can now:

- Look up flight prices
- Analyze uploaded data
- Automate workflows (e.g., calendar booking, API calls)

It‚Äôs no longer just reactive ‚Äî it‚Äôs _proactive_ and _task-capable_.

In the next step, we'll walk through **testing the agent in real-world scenarios** and ensuring robustness. Stay tuned! üìä

## üß™ Step 5: Testing the Agent ‚Äì Real-World Scenarios

Now that our assistant can handle multi-modal input and use tools, it‚Äôs crucial to **validate its behavior in the wild**. This step focuses on practical testing, edge-case handling, debugging tools, and observability.

Our goal? Build an assistant that‚Äôs not only smart, but also _reliable, explainable, and robust_.

### üî¢ Key Testing Scenarios

Start by validating the assistant in realistic, mixed-input tasks. Here are a few to simulate:

#### 1. üí° Complex Task Decomposition

```text
"Check today‚Äôs weather in Paris, summarize this PDF I uploaded, and read this receipt image."
```

- ‚úÖ Should use SerpAPI for weather
- ‚úÖ Use LangChain tool for PDF
- ‚úÖ GPT-4o vision for image analysis

#### 2. üé§ Voice to Tool Chain

```text
(voice) "What is the capital of Brazil and how many letters does it have?"
```

- ‚úÖ Whisper transcribes
- ‚úÖ Agent uses internal logic

#### 3. üåç Location + Image

```text
"Here‚Äôs a map screenshot. Which station is closest to the Eiffel Tower?"
```

- ‚úÖ Vision API should analyze image content

#### 4. üî¨ Math & File Combo

```text
"Calculate the average from this Excel file and then plot it."
```

- ‚úÖ Tool reads CSV or Excel
- ‚úÖ Uses chart library or returns summary

### ü§ì Prompt Logging (Highly Recommended)

To debug LLM behavior, **log every message and response**.

In your `api/chat` and `api/agent` handlers:

```ts
console.log("USER PROMPT:", input);
console.log("MODEL RESPONSE:", result);
```

You can optionally save logs in a Firestore or Supabase table.

### üìÉ Response Time Benchmarking

Wrap your executor call with time measurement:

```ts
const start = Date.now();
const result = await runAgent(input);
const end = Date.now();
console.log(`‚è±Ô∏è Agent took ${end - start}ms`);
```

This helps catch latency regressions when adding more tools.

### üåê Frontend UX Tips for Testing

Add feedback components:

- ‚è≥ Loading spinners
- ‚úâ Transcribed voice bubble
- ‚Ñπ Agent explanation popup ("I used SerpAPI and Calculator")

Also add retry buttons if a response fails.

### üß∞ Handling Failures Gracefully

Agents can fail or hallucinate. You can mitigate that:

#### Fallback Messages

```ts
if (!result) {
  return NextResponse.json({
    result: "Sorry, I couldn't complete that request.",
  });
}
```

#### Timeout Guards

```ts
const timeoutPromise = new Promise((_, reject) =>
  setTimeout(() => reject("Timed out"), 8000)
);

const result = await Promise.race([runAgent(input), timeoutPromise]);
```

#### Input Filtering

Reject unsupported inputs early:

```ts
if (!input && !file) {
  return NextResponse.json({ error: "Empty input" });
}
```

### üìä Summary Table: Test Plan

| Scenario              | Features Tested             | Expected Output                      |
| --------------------- | --------------------------- | ------------------------------------ |
| Weather + PDF + Image | Tool chaining + multi-modal | Text + visual summary                |
| Voice Question        | Whisper + GPT               | TTS output                           |
| Image Map + Query     | Vision understanding        | Closest landmark or captioned info   |
| Excel Analysis        | File reader + math tool     | Avg, stats, visual or tabular result |

### üöÄ You‚Äôre Almost There

These real-world test cases will help you fine-tune and build confidence in your assistant‚Äôs abilities. Next, we‚Äôll walk through **deploying your AI assistant to Vercel**, including rate-limit handling, performance tuning, and edge functions. ‚ú®
