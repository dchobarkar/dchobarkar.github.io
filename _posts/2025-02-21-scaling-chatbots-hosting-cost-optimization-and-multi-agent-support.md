# Conversational AI: Building Intelligent Chatbots Across Platforms - 09: Scaling Chatbots: Hosting, Cost Optimization, and Multi-Agent Support

## ğŸš€ Project Setup for Scalability

Before scaling a chatbot to production-grade levels, you need a solid foundation. This starts with a modular, maintainable codebase and a structure that supports growth without collapsing under complexity. In this section, we'll scaffold a scalable chatbot server using **TypeScript + Node.js**, designed to be hosted on platforms like Railway, Vercel, or even Docker containers.

We'll use a clean monorepo-style layout (not necessarily with Nx or Turborepo unless complexity demands it), with focus on modular folders like `agents/`, `routes/`, `services/`, and `utils/`.

### ğŸ“ Folder Structure

Here's the recommended structure for a multi-agent chatbot setup:

```structure
chatbot-scalable-infra/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ supportAgent.ts
â”‚   â””â”€â”€ salesAgent.ts
â”œâ”€â”€ routes/
â”‚   â””â”€â”€ chatbot.route.ts
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ openai.service.ts
â”‚   â”œâ”€â”€ memory.service.ts
â”‚   â””â”€â”€ agentManager.service.ts
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.ts
â”œâ”€â”€ app.ts
â”œâ”€â”€ server.ts
â”œâ”€â”€ types/
â”‚   â””â”€â”€ index.d.ts
â”œâ”€â”€ .env
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

Each part of the system will be encapsulated so you can plug in or replace components (like agents or memory logic) independently.

### ğŸ§± Base Setup â€“ `package.json`

Create a new Node.js + TypeScript project:

```bash
npm init -y
npm install express dotenv openai
npm install -D typescript ts-node-dev @types/node @types/express
```

Then, set up `tsconfig.json`:

```json
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "commonjs",
    "strict": true,
    "esModuleInterop": true,
    "outDir": "dist",
    "baseUrl": ".",
    "paths": {
      "@agents/*": ["agents/*"],
      "@services/*": ["services/*"],
      "@utils/*": ["utils/*"]
    }
  },
  "include": ["**/*.ts"],
  "exclude": ["node_modules"]
}
```

### ğŸŒ Entry Point â€“ `server.ts`

This file boots up the Express server and loads routes.

```ts
// server.ts
import express from "express";
import dotenv from "dotenv";
import chatbotRoutes from "./routes/chatbot.route";
import { logger } from "./utils/logger";

dotenv.config();

const app = express();
const port = process.env.PORT || 3000;

app.use(express.json());
app.use("/api/chat", chatbotRoutes);

app.listen(port, () => {
  logger.info(`Chatbot server running at http://localhost:${port}`);
});
```

### ğŸ¤– Agent Example â€“ `agents/supportAgent.ts`

Each agent handles its own prompts and logic:

```ts
// agents/supportAgent.ts
import { ChatCompletionRequestMessage } from "openai";

export const supportAgentPrompt: ChatCompletionRequestMessage[] = [
  {
    role: "system",
    content:
      "You are a friendly customer support agent. Answer questions politely and clearly.",
  },
];
```

### ğŸ“¡ Route Layer â€“ `routes/chatbot.route.ts`

```ts
// routes/chatbot.route.ts
import { Router } from "express";
import { handleChatRequest } from "../services/agentManager.service";

const router = Router();

router.post("/:agentId", handleChatRequest);

export default router;
```

### ğŸ§  Agent Manager â€“ `services/agentManager.service.ts`

```ts
// services/agentManager.service.ts
import { Request, Response } from "express";
import { supportAgentPrompt } from "../agents/supportAgent";
import { askOpenAI } from "./openai.service";

const agentMap: Record<string, any> = {
  support: supportAgentPrompt,
};

export const handleChatRequest = async (req: Request, res: Response) => {
  const agentId = req.params.agentId;
  const userMessage = req.body.message;

  if (!agentMap[agentId]) {
    return res.status(404).json({ error: "Agent not found" });
  }

  const prompt = [...agentMap[agentId], { role: "user", content: userMessage }];

  try {
    const reply = await askOpenAI(prompt);
    res.json({ reply });
  } catch (err) {
    res.status(500).json({ error: "Failed to process message" });
  }
};
```

### âš™ï¸ OpenAI Wrapper â€“ `services/openai.service.ts`

```ts
// services/openai.service.ts
import { Configuration, OpenAIApi, ChatCompletionRequestMessage } from "openai";

const config = new Configuration({ apiKey: process.env.OPENAI_API_KEY });
const openai = new OpenAIApi(config);

export const askOpenAI = async (messages: ChatCompletionRequestMessage[]) => {
  const response = await openai.createChatCompletion({
    model: "gpt-3.5-turbo",
    messages,
  });
  return response.data.choices[0].message?.content;
};
```

### ğŸ› ï¸ Logger Utility â€“ `utils/logger.ts`

```ts
// utils/logger.ts
export const logger = {
  info: (...args: any[]) => console.log("[INFO]", ...args),
  error: (...args: any[]) => console.error("[ERROR]", ...args),
};
```

### âœ… Running the Bot

Add the following to your `package.json`:

```json
"scripts": {
  "dev": "ts-node-dev server.ts"
}
```

Create a `.env` file:

```env
OPENAI_API_KEY=your-api-key
```

Run the dev server:

```bash
npm run dev
```

Make a POST request to `http://localhost:3000/api/chat/support` with `{ "message": "Hello!" }` and you'll get a response from the support agent ğŸš€

## ğŸŒ Hosting on Railway with Environment Isolation

Hosting your chatbot on **Railway** is a developer-friendly way to manage scalable infrastructure with built-in environment management, observability, and CI/CD. In this section, we'll deploy the chatbot we built in the previous section to Railway, configure isolated environments, manage secrets, and enable auto-deploy from GitHub.

### ğŸš‰ Why Railway?

- **Simplicity**: Deploy full-stack apps without managing Docker or cloud config.
- **CI/CD Integration**: Auto-deploy on push to GitHub.
- **Environment Isolation**: Separate Dev, Staging, and Prod setups.
- **Secrets Management**: Encrypted environment variable support.
- **Scalability**: Railway autoscales based on usage.

### ğŸ§ª Step-by-Step Deployment Guide

#### 1. ğŸ›  Prepare Your Repo

Create a GitHub repo named `chatbot-scalable-infra` and push your code:

```bash
git init
git remote add origin https://github.com/your-username/chatbot-scalable-infra.git
git add .
git commit -m "Initial commit"
git push -u origin main
```

Ensure your `package.json` has:

```json
"scripts": {
  "start": "node dist/server.js",
  "build": "tsc"
}
```

#### 2. ğŸš€ Connect to Railway

1. Go to [https://railway.app](https://railway.app)
2. Click `New Project â†’ Deploy from GitHub Repo`
3. Select your `chatbot-scalable-infra` repo
4. Set Build Command: `npm run build`
5. Set Start Command: `npm start`
6. Railway auto-detects the `server.ts` output after build

#### 3. ğŸ” Add Environment Variables

Navigate to the **Variables** tab:

```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
PORT=3000
```

You can define **Environment Groups** to manage shared secrets across projects.

#### 4. ğŸ§ª Enable Multiple Environments

1. Go to project settings â†’ Environments
2. Add new environments like `staging` and `prod`
3. Set unique env vars for each environment
4. Railway separates deployments and variables per environment

You can create environment-specific `.env.staging` or `.env.prod` locally for testing.

#### 5. âœ… Auto Deploy with GitHub Actions

Railway sets this up automatically, but you can customize it:

```yml
# .github/workflows/railway-deploy.yml
name: Deploy to Railway

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: railwayapp/railway-deploy-action@v1.2.0
        with:
          railwayToken: ${{ secrets.RAILWAY_TOKEN }}
          projectId: your_project_id_here
```

Generate the Railway token from your user settings and add it to GitHub Actions secrets.

### ğŸ” Debugging Logs and Deployment Status

Railway gives you a real-time terminal view:

- Navigate to **Deployments** tab
- Click on any build to view logs
- Errors are shown clearly (build, runtime, port issues)

You can restart or redeploy from the dashboard at any time.

### ğŸ§¼ Production Hardening Tips

- **Set NODE_ENV to `production`** in env vars
- Use Railway's built-in metrics dashboard to monitor usage
- Set up custom domains under Railwayâ€™s domain tab
- Enable alerts or email notifications for failures

```bash
NODE_ENV=production
```

### ğŸ” Redeploying After Changes

Every time you push to `main`, Railway will:

1. Pull changes from GitHub
2. Run `npm run build`
3. Start the app using `node dist/server.js`
4. Apply environment variables

Use `railway up` via CLI for local testing with your Railway project's environment:

```bash
npm i -g railway
railway login
railway link
railway run npm run dev
```

ğŸ‰ With this, your chatbot is now production-ready and hosted on a scalable Railway infrastructure. You can now build on this by connecting databases, Redis, or third-party APIs from Railwayâ€™s plugin marketplace!

## ğŸ§  Persistent Vector Store for Shared Memory

One of the key challenges in building intelligent chatbots is **memory** â€” the ability to retain and recall context across sessions. To achieve this at scale, we can use **vector databases** like Supabase (with pgvector), Pinecone, or Weaviate. In this section, weâ€™ll:

- Integrate **Supabase + pgvector** into our chatbot
- Store and retrieve memory embeddings
- Optimize prompts using vector similarity

### ğŸ¤” Why Vector Stores?

Traditional databases are great for exact matches. But for conversational memory, we need **semantic search**:

- Compare user queries with previous interactions
- Match intent and meaning, not just keywords
- Enable long-term context recall across sessions

Vector databases allow storing high-dimensional **embeddings** (numerical representations of text) and running **similarity searches** (e.g., cosine distance).

### ğŸ§° Choosing Supabase + pgvector

Weâ€™ll use Supabase because:

- It's easy to set up with pgvector
- Offers PostgreSQL + API + Auth in one platform
- Supports full-text + vector hybrid queries

### ğŸ§± Set Up Supabase with pgvector

1. Create a project at [https://app.supabase.com](https://app.supabase.com)
2. Enable `pgvector` extension:

   ```sql
   create extension if not exists vector;
   ```

3. Create a `memory` table:

```sql
create table memory (
  id uuid primary key default gen_random_uuid(),
  session_id text,
  role text,
  content text,
  embedding vector(1536),
  created_at timestamp default now()
);
```

### ğŸ”Œ Install Supabase Client

```bash
npm install @supabase/supabase-js
```

Add to `.env`:

```env
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-anon-key
```

### ğŸ§  memory.service.ts â€“ Embed, Store, Search

```ts
// services/memory.service.ts
import { createClient } from "@supabase/supabase-js";
import { Configuration, OpenAIApi } from "openai";

const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_ANON_KEY!
);
const openai = new OpenAIApi(
  new Configuration({ apiKey: process.env.OPENAI_API_KEY })
);

const embedText = async (text: string) => {
  const res = await openai.createEmbedding({
    model: "text-embedding-ada-002",
    input: text,
  });
  return res.data.data[0].embedding;
};

export const storeMessage = async (
  sessionId: string,
  role: string,
  content: string
) => {
  const embedding = await embedText(content);
  await supabase
    .from("memory")
    .insert({ session_id: sessionId, role, content, embedding });
};

export const fetchSimilarMessages = async (
  sessionId: string,
  userInput: string,
  limit = 5
) => {
  const queryEmbedding = await embedText(userInput);
  const { data, error } = await supabase.rpc("match_memory", {
    query_embedding: queryEmbedding,
    match_threshold: 0.78,
    match_count: limit,
  });
  return data || [];
};
```

### ğŸ§  PostgreSQL Function for Similarity â€“ `match_memory`

```sql
create or replace function match_memory(
  query_embedding vector,
  match_threshold float,
  match_count int
)
returns table (
  id uuid,
  session_id text,
  role text,
  content text,
  similarity float
) as $$
  select id, session_id, role, content,
         1 - (embedding <=> query_embedding) as similarity
  from memory
  where 1 - (embedding <=> query_embedding) > match_threshold
  order by similarity desc
  limit match_count;
$$ language sql;
```

### ğŸ¤– Using Memory in the Agent

Update `agentManager.service.ts`:

```ts
import { fetchSimilarMessages, storeMessage } from "./memory.service";

const history = await fetchSimilarMessages(sessionId, userMessage);
const prompt = [
  ...agentMap[agentId],
  ...history.map((h) => ({ role: h.role, content: h.content })),
  { role: "user", content: userMessage },
];

const reply = await askOpenAI(prompt);
await storeMessage(sessionId, "user", userMessage);
await storeMessage(sessionId, "assistant", reply);
```

### ğŸ’¡ Tips for Scalable Vector Memory

- Use batch insert to reduce Supabase calls
- Trim old memories or cap per session
- Use hybrid filters: `session_id + similarity`
- Prefer embedding short chunks (~100-200 tokens)

With this setup, your chatbot now has **long-term memory**, backed by semantic search, and scalable for multi-session use cases across different users and agents ğŸš€

## âš¡ Rate Limiting, Caching, and Token Usage Optimization

When running chatbots in production, managing **API costs** and **response latency** becomes crucial â€” especially when using models like GPT-4. In this section, weâ€™ll implement:

- Rate limiting per user/session
- Embedding + response caching with Redis
- Token usage analysis and optimization techniques

### ğŸš§ Why This Matters

- OpenAI charges per token â€” every token counts ğŸ“‰
- Without caching, repeated prompts incur redundant costs
- Spamming or flooding can burn API quota and degrade UX

### ğŸ§± Dependencies

Install required packages:

```bash
npm install express-rate-limit ioredis gpt-tokenizer
```

Add Redis to your Railway project or run it locally:

```env
REDIS_URL=redis://default:<password>@<host>:<port>
```

### ğŸ§¼ Rate Limiting Middleware

```ts
// middleware/rateLimiter.ts
import rateLimit from "express-rate-limit";

export const rateLimiter = rateLimit({
  windowMs: 1 * 60 * 1000, // 1 minute
  max: 30, // limit each IP to 30 requests per minute
  standardHeaders: true,
  legacyHeaders: false,
});
```

Apply in `server.ts`:

```ts
import { rateLimiter } from "./middleware/rateLimiter";
app.use(rateLimiter);
```

### ğŸ”„ Redis Cache Wrapper

```ts
// services/cache.service.ts
import Redis from "ioredis";
const redis = new Redis(process.env.REDIS_URL!);

const hashKey = (key: string) => `cache:${Buffer.from(key).toString("base64")}`;

export const getCachedResponse = async (key: string) => {
  return await redis.get(hashKey(key));
};

export const setCachedResponse = async (
  key: string,
  value: string,
  ttl = 60 * 5
) => {
  await redis.set(hashKey(key), value, "EX", ttl);
};
```

### ğŸ¤– Caching OpenAI Calls

Update `openai.service.ts`:

```ts
import { getCachedResponse, setCachedResponse } from "./cache.service";

export const askOpenAI = async (messages: ChatCompletionRequestMessage[]) => {
  const promptKey = JSON.stringify(messages);
  const cached = await getCachedResponse(promptKey);
  if (cached) return cached;

  const response = await openai.createChatCompletion({
    model: "gpt-3.5-turbo",
    messages,
  });

  const reply = response.data.choices[0].message?.content || "";
  await setCachedResponse(promptKey, reply);
  return reply;
};
```

### ğŸ” Token Counting & Optimization

Use `gpt-tokenizer` to count prompt size:

```ts
// utils/tokenUtils.ts
import { encoding_for_model } from "gpt-tokenizer";

const encoder = encoding_for_model("gpt-3.5-turbo");

export const countTokens = (text: string): number => {
  return encoder.encode(text).length;
};
```

In agent logic:

```ts
const totalTokens = prompt.reduce(
  (acc, msg) => acc + countTokens(msg.content),
  0
);
if (totalTokens > 3500) {
  prompt.splice(1, 1); // Trim early history or simplify system prompt
}
```

### ğŸ’¡ Optimization Strategies

- **Use GPT-3.5 over GPT-4** unless quality absolutely demands it
- **Avoid redundant system prompts** for every request
- **Trim old conversation history** or summarize periodically
- **Chunk inputs** (e.g. document Q&A) to avoid large token burst
- **Cache embeddings** for duplicate queries or similar vectors

With this setup:

- You're limiting abuse via rate control
- Saving costs by caching repeated OpenAI calls
- Monitoring and trimming excessive token usage

All while preserving a responsive and intelligent chatbot experience âœ¨

## ğŸ§  Multi-Agent Architecture: Serving Different Bots from Same Server

As your chatbot infrastructure grows, youâ€™ll often need to run **multiple bots** â€” such as a support agent, a sales agent, and maybe even a developer advocate bot â€” all from the **same backend**. This section shows how to implement a flexible **multi-agent architecture** that supports modularity, isolation, and easy scaling.

### ğŸ¯ Goals

- Dynamically route requests to the correct agent logic
- Keep agent behavior encapsulated and modular
- Enable per-agent prompt strategy, memory, and OpenAI settings

### ğŸ“¦ Agent Interface Design

Define a common interface for agents:

```ts
// types/Agent.ts
import { ChatCompletionRequestMessage } from "openai";

export interface Agent {
  id: string;
  label: string;
  systemPrompt: string;
  processMessage(userMessage: string, sessionId: string): Promise<string>;
}
```

### ğŸ¤– Implement Agents Separately

Each agent implements its own logic, prompt, and tools:

```ts
// agents/supportAgent.ts
import { Agent } from "../types/Agent";
import { askOpenAI } from "../services/openai.service";
import { storeMessage, fetchSimilarMessages } from "../services/memory.service";

export const supportAgent: Agent = {
  id: "support",
  label: "Support Bot",
  systemPrompt: "You are a helpful support assistant.",

  async processMessage(userMessage, sessionId) {
    const history = await fetchSimilarMessages(sessionId, userMessage);
    const messages = [
      { role: "system", content: this.systemPrompt },
      ...history.map((h) => ({ role: h.role, content: h.content })),
      { role: "user", content: userMessage },
    ];
    const reply = await askOpenAI(messages);
    await storeMessage(sessionId, "user", userMessage);
    await storeMessage(sessionId, "assistant", reply);
    return reply;
  },
};
```

```ts
// agents/salesAgent.ts
import { Agent } from "../types/Agent";
import { askOpenAI } from "../services/openai.service";

export const salesAgent: Agent = {
  id: "sales",
  label: "Sales Assistant",
  systemPrompt:
    "You are a persuasive sales assistant who converts leads into customers.",

  async processMessage(userMessage, sessionId) {
    const messages = [
      { role: "system", content: this.systemPrompt },
      { role: "user", content: userMessage },
    ];
    return await askOpenAI(messages);
  },
};
```

### ğŸ§  Agent Manager

Create a dynamic registry of agents:

```ts
// services/agentRegistry.ts
import { supportAgent } from "../agents/supportAgent";
import { salesAgent } from "../agents/salesAgent";
import { Agent } from "../types/Agent";

const agents: Record<string, Agent> = {
  [supportAgent.id]: supportAgent,
  [salesAgent.id]: salesAgent,
};

export const getAgentById = (id: string): Agent | null => agents[id] || null;
export const listAgents = (): Agent[] => Object.values(agents);
```

### ğŸ“¡ Unified Endpoint

Update your router to support dynamic agents:

```ts
// routes/chatbot.route.ts
import { Router } from "express";
import { getAgentById } from "../services/agentRegistry";

const router = Router();

router.post("/:agentId", async (req, res) => {
  const { agentId } = req.params;
  const { message, sessionId } = req.body;
  const agent = getAgentById(agentId);

  if (!agent) return res.status(404).json({ error: "Agent not found" });

  try {
    const reply = await agent.processMessage(message, sessionId);
    res.json({ reply });
  } catch (e) {
    res.status(500).json({ error: "Failed to process message" });
  }
});

export default router;
```

### ğŸ§ª Testing the Architecture

Example POST request:

```json
POST /api/chat/support
{
  "message": "How do I reset my password?",
  "sessionId": "user-123"
}
```

You can easily test different agents by changing the endpoint:

- `/api/chat/support`
- `/api/chat/sales`

### ğŸ§  Benefits of Multi-Agent Design

- **Modular Logic**: Each agent is a self-contained module
- **Easy Extension**: Add more agents without modifying core logic
- **Per-Agent Optimization**: Tune prompts, memory, caching separately
- **Centralized Control**: Shared infrastructure, separate intelligence

This structure is ideal for teams building **multi-domain bots** â€” customer support, HR assistants, sales reps â€” all running from the same backend but tailored to different audiences and tasks âœ¨

## ğŸ•µï¸â€â™‚ï¸ Observability: Logs, Metrics, and Alerts

Once your chatbot is live and serving users, observability becomes critical. You need to:

- Monitor uptime and performance
- Track user interactions and error rates
- Be alerted when something breaks (before users complain)

In this section, weâ€™ll implement structured logging, basic metrics, and alerts using both built-in tools (like Railwayâ€™s dashboard) and external services like Grafana + Prometheus or simple Slack/email notifications.

### ğŸ§¾ 1. Structured Logging with Winston

Use `winston` for production-ready, formatted logs.

```bash
npm install winston
```

```ts
// utils/logger.ts
import winston from "winston";

export const logger = winston.createLogger({
  level: "info",
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [new winston.transports.Console()],
});
```

Usage:

```ts
logger.info("Incoming request", { agentId, sessionId, userMessage });
logger.error("OpenAI failed", { error });
```

### ğŸ“ˆ 2. Metrics with Prometheus + Express Middleware

Use `prom-client` to expose Prometheus-compatible metrics.

```bash
npm install prom-client
```

```ts
// metrics/prometheus.ts
import client from "prom-client";

client.collectDefaultMetrics();

export const messageCounter = new client.Counter({
  name: "chatbot_messages_total",
  help: "Total number of messages handled",
});

export const tokenUsageGauge = new client.Gauge({
  name: "chatbot_tokens_used",
  help: "Estimated total tokens used",
});

export const metricsMiddleware = async (_req, res) => {
  res.set("Content-Type", client.register.contentType);
  res.end(await client.register.metrics());
};
```

Update `server.ts`:

```ts
import { metricsMiddleware } from "./metrics/prometheus";
app.get("/metrics", metricsMiddleware);
```

### ğŸ“¬ 3. Alerting via Slack or Email

Use a simple webhook to alert on errors or high traffic:

```ts
// utils/alert.ts
import axios from "axios";

export const sendAlert = async (message: string) => {
  try {
    await axios.post(process.env.ALERT_WEBHOOK_URL!, {
      text: `[Chatbot Alert] ${message}`,
    });
  } catch (err) {
    console.error("Alert failed", err);
  }
};
```

Trigger on critical paths:

```ts
try {
  // logic
} catch (err) {
  logger.error("Agent processing failed", { error: err });
  await sendAlert("Agent processing failed: " + err.message);
}
```

Use Slack, Discord, or services like Pushover/email gateways for the webhook.

### ğŸ” 4. Railway Dashboard Tools

Railwayâ€™s UI gives you:

- Real-time logs per environment
- CPU/Memory usage per instance
- Deployment history
- Built-in metrics tab (if enabled)

Enable log-based alerts under Railway project settings:

- Add keywords like `error`, `failed`, or `timeout`
- Set notification targets

### ğŸš¦ 5. Health Check Endpoint

```ts
// routes/health.route.ts
import { Router } from "express";
const router = Router();

router.get("/", (_req, res) => {
  res.json({ status: "ok", timestamp: new Date().toISOString() });
});

export default router;
```

Mount it:

```ts
import healthRoute from "./routes/health.route";
app.use("/health", healthRoute);
```

This lets Railway, Render, or uptime monitors ping and validate your bot.

### ğŸ§  Bonus: Grafana + Railway Logs

You can export logs via a webhook or to a cloud function, and stream them into a custom Grafana dashboard.

- Use Railwayâ€™s plugin marketplace or third-party log streaming
- Or deploy a middleware that ships logs to Loki (Grafana stack)

With these tools in place, your chatbot becomes **observable, debuggable, and resilient** â€” able to notify you of issues, measure performance, and improve continuously ğŸ”ğŸ“Š
